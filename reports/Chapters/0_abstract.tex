\begin{abstract}
    % 1. The Problem 2. The Issues 3. The Approach 4. The Results 5. The Experiments 6. The Applications
    Distributional word representation such as GloVe and BERT has garnered immense popularity and research interest in recent years due to their success in many downstream NLP applications. However, a major limitation of word embedding is its inability to handle unknown words. To make sense of words that were not present in training, current NLP models use sub-word embedding (obtained via sub-word segmentation algorithms), however, this approach often fails to capture the semantic sense of the word due to words being broken down in a syntactic manner. There has been other approaches to tackle embedding of unknown words using ConceptNet and Recursive Neural Network, but did not enjoy much usage due to their complexities in design. This paper presents a novel solution to generate embedding for OOV using a neural rather than symbolic approach. This approach capitalizes on the existing semantics captured in known words' embedding and trains a simple feed-forward neural network to capture the compositionality function of embedding in their latent space. Linguistic studies have shown that the interested compositionality function is broad and varied, therefore this paper introduces a preliminary study into the compositionality of noun, with focus on certain named entities.  The trained network is able to generate an embedding for an unknown word based on its context words, which can be obtained via crawling of web data. This synthetic embedding can then be incorporated into the embedding matrix of existing application. From our experiments, we can conclude that [include when more results are out]
\end{abstract}