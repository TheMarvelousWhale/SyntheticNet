{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a804edf9-aa22-4edd-9c30-7781b3f72d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Sample: \n",
    "        config.data = \"wiki_data, word cats and pneumonia\"\n",
    "        config.loss = \"L1\"\n",
    "        config.simscore = \"\"\n",
    "        config.batch_size\n",
    "        config.epoch\n",
    "        config.lr\n",
    "        config.momentum (since we are using SGD)\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e37ede-b7a2-4664-8eb1-0322acd8d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: sosig_catto (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">clear-haze-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sosig_catto/Synthetic%20Net%20Roberta\" target=\"_blank\">https://wandb.ai/sosig_catto/Synthetic%20Net%20Roberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sosig_catto/Synthetic%20Net%20Roberta/runs/2yvl8wgr\" target=\"_blank\">https://wandb.ai/sosig_catto/Synthetic%20Net%20Roberta/runs/2yvl8wgr</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\hoang\\Desktop\\Light\\Torch_playground\\FYPv2\\notebooks\\wandb\\run-20211001_235102-2yvl8wgr</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import wandb\n",
    "import re\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "wandb.init(project='Synthetic Net Roberta')\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "434a9350-afbe-47bf-801d-ee6d026904f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (act): ReLU()\n",
      "  (out): Tanh()\n",
      "  (hidden1): Linear(in_features=15360, out_features=2048, bias=True)\n",
      "  (hidden2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (hidden3): Linear(in_features=512, out_features=768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,context_length,embed_size=100):\n",
    "        super().__init__()\n",
    "        self.n = context_length*2\n",
    "        self.embed_size = embed_size\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Tanh() \n",
    "        self.hidden1 = nn.Linear(self.n*self.embed_size,2048)\n",
    "        self.hidden2 = nn.Linear(2048,512)\n",
    "        self.hidden3 = nn.Linear(512,self.embed_size)\n",
    " \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.act(self.hidden1(x))\n",
    "        x = self.act(self.hidden2(x))\n",
    "        x = self.out(self.hidden3(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "synmodel = DenseNet(context_length = 10,embed_size=768)\n",
    "print(synmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998853be-3300-4dc8-a419-9b92b95fa3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#setup roberta\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch\n",
    "checkpoint='roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "323d1486-b616-499e-bc66-021af5352ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.vocab\n",
    "roberta_embeddings = model.embeddings.word_embeddings.weight\n",
    "special_token_ids = tokenizer.convert_tokens_to_ids([tokenizer.pad_token,tokenizer.unk_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ef234ab-a3d1-4f02-8463-99d58c0edb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_embeddings[[1,2,3]].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc422be-408d-4b09-97b0-b24f4e41ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "\n",
    "#Loading the data\n",
    "    \n",
    "config.batch_size = 64\n",
    "\n",
    "corpus = 'giga'\n",
    "\n",
    "training_files = [f'../processed_data/{corpus}_only/{x}_corpus_stopwords_c10.txt' for x in ['disease','pneumonia','vaccine','virus','sick']]\n",
    "training_data = load_training_batch(training_files,config.batch_size)\n",
    "#config.data = \"wiki_only_5words-covid\"\n",
    "def token_to_id(word):\n",
    "    if word in vocab:\n",
    "        return vocab[word]\n",
    "    elif word == tokenizer.pad_token:\n",
    "        return special_token_ids[0]\n",
    "    else:\n",
    "        return special_token_ids[1]\n",
    "        \n",
    "def get_roberta_embedding(train_data):\n",
    "    train_tensor = []\n",
    "    for i,batch in enumerate(train_data):\n",
    "        batch_tensor_list = []\n",
    "        batch_label_list = []\n",
    "        for sentence in batch:\n",
    "            try:\n",
    "                y,x = sentence.split(':')\n",
    "                s = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "                input_ids = [token_to_id(word) for word in s.split(' ')]\n",
    "                context_embeddings = roberta_embeddings[input_ids]\n",
    "                batch_tensor_list.append(context_embeddings)\n",
    "                y_id = token_to_id(y)\n",
    "                label_embedding = roberta_embeddings[y_id]\n",
    "                batch_label_list.append(label_embedding)\n",
    "            except Exception as e:\n",
    "                print(sentence)\n",
    "                print(e)\n",
    "                return\n",
    "        train_tensor.append((torch.stack(batch_tensor_list),torch.stack(batch_label_list)))\n",
    "    return train_tensor\n",
    "\n",
    "#list of tuples of tensors\n",
    "\n",
    "train_tensor = get_roberta_embedding(training_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a274f19-a1a8-4058-bd96-781719c294ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "torch.Size([64, 20, 768])\n",
      "torch.Size([64, 768])\n",
      "torch.Size([20, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tensor))   #--num batch\n",
    "print(len(train_tensor[0]))   #tuple\n",
    "print(train_tensor[0][0].size())  #input_ids\n",
    "print(train_tensor[0][1].size())  #label_ids\n",
    "print(train_tensor[0][0][0].size())  #input_ids\n",
    "print(train_tensor[0][1][0].size())   #batch 0, label, first_eg of that batch 's label\n",
    "print(train_tensor[0][0][0][0].size()) #batch 0, input_ids, first_eg of that batch -- 20 tensors, choose the first one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbdf3ac8-41c1-4f3c-b4a0-6347656ba473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "config.lr = 0.0005\n",
    "config.momentum = 0.005\n",
    "optimizer = optim.SGD(model.parameters(),lr=config.lr,momentum=config.momentum,weight_decay=0.01)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def cosim(v1,v2):\n",
    "    return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, debug_set.shape[0], eta_min=config.lr)\n",
    "#learning rate adjustment -- try 0.001\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        features,labels = batch\n",
    "        feat = features\n",
    "        label = labels\n",
    "        print(feat.size())\n",
    "        print(label.size())\n",
    "        predictions = model(feat).squeeze(1)\n",
    "        loss = criterion(predictions,label)  \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        cosim_score = np.mean([cosim(labels[i].detach().numpy(),predictions[i].detach().numpy()) for i in range(config.batch_size) ])\n",
    "        \n",
    "    return epoch_loss,cosim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad37eec9-4584-4e7b-a6b8-8d633f1b2538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 20, 768])\n",
      "torch.Size([64, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:01\t|\tTrain Loss: 0.029\t|\tCosim score: -0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "config.epochs = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(config.epochs)):   \n",
    "    train_loss,cosim_score= train(synmodel,iter(train_tensor), optimizer, criterion)\n",
    "\n",
    "    #epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    #if valid_loss < best_valid_loss:\n",
    "     #   best_valid_loss = valid_loss\n",
    "      #  torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    #print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    wandb.log({\"loss\":train_loss,\"cosim_score\":cosim_score})\n",
    "    print(f'Epoch:{epoch+1:02}\\t|\\tTrain Loss: {train_loss:.3f}\\t|\\tCosim score: {cosim_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e7306f-0ebe-448a-b28c-46e5ad8f3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files_available = [x[:x.find('_')] for x in os.listdir(f'../processed_data/{corpus}_only/') if x.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "904f563d-2e33-46ac-837f-5787a172934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sample(target,files,n=4):\n",
    "    negatives = [x for x in files if x != target]\n",
    "    return [target] + random.sample(negatives,n)\n",
    "\n",
    "d = {x:negative_sample(x,files_available,2) for x in ['disease','pneumonia','sick']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f38190-aa52-4f2b-9943-1cca6b80ed34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'disease': ['disease', 'pneumonia', 'vaccine'],\n",
       " 'pneumonia': ['pneumonia', 'virus', 'sick'],\n",
       " 'sick': ['sick', 'pneumonia', 'disease']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3119cd0-0466-4409-bd09-0f11a137d061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
