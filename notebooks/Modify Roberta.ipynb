{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e42bb4e6-fed9-47a9-b1db-f4c0dc9925a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch\n",
    "checkpoint='roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c7283-dd02-4d0e-9989-97d151705b76",
   "metadata": {},
   "source": [
    "## Before modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30cd81b5-ee96-4fc4-9e12-ac8f5fd3bbb9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  {'input_ids': tensor([[   0, 4717,  594,    5, 4758,   66,    9,  259,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "\n",
      "Output:  tensor([[[-0.0431,  0.0622, -0.0273,  ..., -0.0284, -0.0659, -0.0054],\n",
      "         [ 0.0162,  0.2585,  0.3546,  ...,  0.7177,  0.2241, -0.0208],\n",
      "         [-0.0698,  0.0751,  0.2477,  ..., -0.3056,  0.1697,  0.0822],\n",
      "         ...,\n",
      "         [ 0.0429, -0.3221, -0.0973,  ..., -0.4146, -0.0282,  0.3446],\n",
      "         [ 0.1115, -0.0527, -0.0417,  ...,  0.5172,  0.0404,  0.1875],\n",
      "         [-0.0341,  0.0536, -0.0554,  ..., -0.0705, -0.0627, -0.0357]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer([\"yeet the cat out of here\"],padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(\"Input: \",batch)\n",
    "output = model(**batch)\n",
    "print(\"\\n\\nOutput: \",output['last_hidden_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef2573-7eb1-4309-a2f8-4658650c59e8",
   "metadata": {},
   "source": [
    "## After modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2cb4c7bb-6e5f-4124-a716-e34832577eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assume we gonna add the word  yeet into vocab and we have the embedding from our network\n",
    "yeet_embed = torch.rand(768).reshape(1,768)\n",
    "\n",
    "new_embedding_layer = torch.cat((a,new_embed))\n",
    "\n",
    "tokenizer.add_tokens('yeet')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.embeddings.word_embeddings.weight = torch.nn.Parameter(new_embedding_layer,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6c7125b9-492e-414d-98bb-fdf7fba9b496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  {'input_ids': tensor([[    0, 50265,     5,  4758,    66,     9,   259,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "\n",
      "Output:  tensor([[[-0.0427,  0.0577, -0.0447,  ..., -0.0186, -0.0503,  0.0067],\n",
      "         [-0.0822, -0.2524, -0.2250,  ...,  0.1061,  0.1537,  0.1627],\n",
      "         [-0.2247, -0.2411, -0.0886,  ..., -0.1973, -0.0052, -0.2409],\n",
      "         ...,\n",
      "         [ 0.0658, -0.2896, -0.0971,  ..., -0.3445, -0.0174,  0.3703],\n",
      "         [ 0.1236,  0.0275, -0.0706,  ...,  0.5766,  0.0081,  0.2497],\n",
      "         [-0.0303,  0.0460, -0.0752,  ..., -0.0514, -0.0464, -0.0197]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer([\"yeet the cat out of here\"],padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(\"Input: \",batch)\n",
    "output = model(**batch)\n",
    "print(\"\\n\\nOutput: \",output['last_hidden_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae2e325-bfd1-4dc6-aa5b-7f6b82bc1985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
