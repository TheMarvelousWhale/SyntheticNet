{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d2a907-65e8-4518-8615-819bfd89037d",
   "metadata": {},
   "source": [
    "Standardize for each experiment:\n",
    "* What data is it run on\n",
    "* What are the loss objective and metrics (cosine sim)\n",
    "* Running epoch and results \n",
    "* Versioning of data and model (offline, use wandb-id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4511203d-a5b5-4aa7-b0d6-87f3174c0e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Sample: \n",
    "        config.data = \"wiki_data, word cats and pneumonia\"\n",
    "        config.loss = \"L1\"\n",
    "        config.simscore = \"\"\n",
    "        config.batch_size\n",
    "        config.epoch\n",
    "        config.lr\n",
    "        config.momentum (since we are using SGD)\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6599d95a-a5e9-4a0c-ad40-81d214f2f861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">crisp-moon-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sosig_catto/Synthetic%20Net%20Data\" target=\"_blank\">https://wandb.ai/sosig_catto/Synthetic%20Net%20Data</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sosig_catto/Synthetic%20Net%20Data/runs/zcztl8m1\" target=\"_blank\">https://wandb.ai/sosig_catto/Synthetic%20Net%20Data/runs/zcztl8m1</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\hoang\\Desktop\\Light\\Torch_playground\\FYPv2\\notebooks\\wandb\\run-20211001_185907-zcztl8m1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import wandb\n",
    "import re\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "wandb.init(project='Synthetic Net Data')\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eda3fba-3bb8-4490-8f38-5909d525ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (act): ReLU()\n",
      "  (out): Tanh()\n",
      "  (hidden1): Linear(in_features=2000, out_features=2048, bias=True)\n",
      "  (hidden2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (hidden3): Linear(in_features=512, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,context_length,embed_size=100):\n",
    "        super().__init__()\n",
    "        self.n = context_length*2\n",
    "        self.embed_size = 100\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Tanh() \n",
    "        self.hidden1 = nn.Linear(self.n*self.embed_size,2048)\n",
    "        self.hidden2 = nn.Linear(2048,512)\n",
    "        self.hidden3 = nn.Linear(512,self.embed_size)\n",
    " \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.act(self.hidden1(x))\n",
    "        x = self.act(self.hidden2(x))\n",
    "        x = self.out(self.hidden3(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = DenseNet(context_length = 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6900d90c-7755-4537-a800-160bfa479ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease:disease:<pad> <pad> <pad> <pad> <pad> <pad> also people worried r repeat sa movement <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "\n",
    "#Loading the data\n",
    "W_norm,vocab,ivocab = load_glove()\n",
    "    \n",
    "config.batch_size = 64\n",
    "\n",
    "corpus = 'giga'\n",
    "\n",
    "training_files = [f'../processed_data/{corpus}_only/{x}_corpus_stopwords_c10.txt' for x in ['disease','pneumonia','vaccine','virus','sick']]\n",
    "training_data = load_training_batch(training_files,config.batch_size)\n",
    "#config.data = \"wiki_only_5words-covid\"\n",
    "\n",
    "def debug_get_embedding(train_data,W_norm,vocab):\n",
    "    train_tensor = []\n",
    "    for i,batch in enumerate(train_data):\n",
    "        tensor_list = []\n",
    "        label_list = []\n",
    "        for sentence in batch:\n",
    "            try:\n",
    "                y,x = sentence.split(':')\n",
    "                s = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "                tensor_list.append([get_glove_vec(word,W_norm,vocab) for word in s.split(' ')])\n",
    "                label_list.append(get_glove_vec(y,W_norm,vocab))\n",
    "            except:\n",
    "                print(sentence)\n",
    "        train_tensor.append((np.array(tensor_list),np.array(label_list)))\n",
    "    return train_tensor\n",
    "\n",
    "\n",
    "train_tensor = debug_get_embedding(training_data,W_norm,vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e93f78-c5e3-43ad-ac97-c31ca911e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "config.lr = 0.0005\n",
    "config.momentum = 0.005\n",
    "optimizer = optim.SGD(model.parameters(),lr=config.lr,momentum=config.momentum,weight_decay=0.01)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def cosim(v1,v2):\n",
    "    return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, debug_set.shape[0], eta_min=config.lr)\n",
    "#learning rate adjustment -- try 0.001\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        features,labels = batch\n",
    "        batch_size = features.shape[0]\n",
    "        predictions = model(torch.Tensor(features)).squeeze(1)\n",
    "        loss = criterion(predictions,torch.Tensor(labels))      \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        cosim_score = np.mean([cosim(labels[i],predictions[i].detach().numpy()) for i in range(batch_size) ])\n",
    "        \n",
    "    return epoch_loss,cosim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77535f77-452b-4ab7-ad45-d5784fd6b1a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                              | 1/50 [01:46<1:26:36, 106.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:01\t|\tTrain Loss: 271.078\t|\tCosim score: 0.426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                            | 2/50 [03:33<1:25:22, 106.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:02\t|\tTrain Loss: 223.222\t|\tCosim score: 0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                            | 2/50 [03:40<1:28:00, 110.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7e46e8944de2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcosim_score\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#epoch_mins, epoch_secs = epoch_time(start_time, end_time)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-7f59997ff782>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mcosim_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcosim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcosim_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-7f59997ff782>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mcosim_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcosim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcosim_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-7f59997ff782>\u001b[0m in \u001b[0;36mcosim\u001b[1;34m(v1, v2)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcosim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, debug_set.shape[0], eta_min=config.lr)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "config.epochs = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(config.epochs)):   \n",
    "    train_loss,cosim_score= train(model,iter(train_tensor), optimizer, criterion)\n",
    "\n",
    "    #epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    #if valid_loss < best_valid_loss:\n",
    "     #   best_valid_loss = valid_loss\n",
    "      #  torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    #print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    wandb.log({\"loss\":train_loss,\"cosim_score\":cosim_score})\n",
    "    print(f'Epoch:{epoch+1:02}\\t|\\tTrain Loss: {train_loss:.3f}\\t|\\tCosim score: {cosim_score:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8939ad7-b8c5-4892-aab2-9cd14095cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'../outputs/{date.today().strftime(\"%Y-%m\")}_{config.data}_{wandb.run.name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca5221bc-5fc1-436f-9b2b-4034e66ea921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_test =  DenseNet(context_length = 10)\n",
    "model_to_test.load_state_dict(torch.load('../outputs/2021-08_wiki_only_pneumonia_winter-firefly-6.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36e44d42-bfdc-4585-9711-8d52d2b596af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "Test 1 -- sample sentence: \n",
      "\n",
      "disease:<pad> <pad> <pad> deborah dainton suffers limp result polio treatment child left claustrophobic reclusive large crowds rigid controlled life transformed \n",
      "\n",
      "\n",
      "\n",
      "(400000,)\n",
      "\n",
      "                               Word       Unnormalized Cosine distance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "0                          pneumonia\t\t0.513979\n",
      "1                         bronchitis\t\t0.422000\n",
      "2                          infection\t\t0.391513\n",
      "3                        respiratory\t\t0.381901\n",
      "4                            illness\t\t0.379707\n",
      "5                         infections\t\t0.376967\n",
      "6                            typhoid\t\t0.373854\n",
      "7                              fever\t\t0.373324\n",
      "8                       tuberculosis\t\t0.371408\n",
      "9                           atypical\t\t0.363049\n",
      "\n",
      "\n",
      "\t\tCosim score: [0.58541959]\n"
     ]
    }
   ],
   "source": [
    "##Testing\n",
    "#Test 1 sentence\n",
    "#Test 1 batch\n",
    "#Test all \n",
    "\n",
    "random_sent = random.choice(training_data)[random.randint(0,config.batch_size-1)]\n",
    "y,x = random_sent.split(':')\n",
    "x = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "sample_tensor = torch.Tensor([[get_glove_vec(word,W_norm,vocab) for word in x.split(' ')]])\n",
    "sample_output = model_to_test(sample_tensor)\n",
    "target_label = np.array(get_glove_vec(y,W_norm,vocab))\n",
    "\n",
    "output = sample_output.squeeze(1)\n",
    "vec_output = output.detach().numpy()\n",
    "print(vec_output.shape)\n",
    "\n",
    "def __distance(W, vocab, ivocab, vec_output):\n",
    "\n",
    "\n",
    "    dist = np.dot(W, vec_output.T).squeeze(1)\n",
    "    print(dist.shape)\n",
    "    a = np.argsort(-dist)[:10]\n",
    "\n",
    "    print(\"\\n                               Word       Unnormalized Cosine distance\\n\")\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    for i,x in enumerate(a):\n",
    "        print(\"%d%35s\\t\\t%f\" % (i,ivocab[str(x)], dist[x]))\n",
    "print(f\"Test 1 -- sample sentence: \\n\\n{random_sent}\\n\\n\")\n",
    "\n",
    "__distance(W_norm,vocab,ivocab,vec_output)\n",
    "\n",
    "\n",
    "print(f\"\\n\\n\\t\\tCosim score: {cosim(vec_output,target_label)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7608b79-baac-4b80-bf1d-5c14ff3c696b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79e2d1-7819-40f0-b542-3b91d946a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_batch = random.choice(training_data)\n",
    "sample_batch_tensor = []\n",
    "target_batch_tensor = []\n",
    "for sentence in random_batch:\n",
    "    y,x = random_sent.split(':')\n",
    "    x = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "    sample_tensor = [get_glove_vec(word,W_norm,vocab) for word in x.split(' ')]\n",
    "    target_batch_tensor.append(get_glove_vec(y,W_norm,vocab))\n",
    "    sample_batch_tensor.append(sample_tensor)\n",
    "    \n",
    "sample_batch_tensor = torch.Tensor(np.array(sample_batch_tensor))\n",
    "target_batch_tensor = np.array(target_batch_tensor)\n",
    "\n",
    "sample_output = model(sample_batch_tensor)\n",
    "\n",
    "output = torch.mean(sample_output,0)   #sum across embeddings\n",
    "vec_output = output.detach().numpy().reshape((1,100))\n",
    "\n",
    "print(f\"Test 2 -- sample batch: \\n\\n\")\n",
    "\n",
    "__distance(W_norm,vocab,ivocab,vec_output)\n",
    "\n",
    "\n",
    "print(f\"\\n\\n\\t\\tCosim score: {cosim(vec_output,target_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3689d-d6c1-4a46-9c80-08bed61816ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Test 3: Custom\n",
    "\n",
    "##Testing\n",
    "#Test 1 sentence\n",
    "#Test 1 batch\n",
    "#Test all \n",
    "\n",
    "\n",
    "random_sent = 'pacific disaster response fund support armenian government fight spread covid year bank committed million loan electric networks armenia ensure electricity '\n",
    "target_word = 'pneumonia'\n",
    "target_label = np.array(get_glove_vec(target_word,W_norm,vocab))\n",
    "random_sent = re.sub('[\\n\\r\\ ]+',' ',random_sent).strip()\n",
    "\n",
    "sample_tensor = torch.Tensor([[get_glove_vec(word,W_norm,vocab) for word in random_sent.split(' ')]])\n",
    "sample_output = model(sample_tensor)\n",
    "output = sample_output.squeeze(1)\n",
    "vec_output = output.detach().numpy()\n",
    "print(f\"Test 3: Custom Test\\n\\n{random_sent}\\n\\n\")\n",
    "__distance(W_norm,vocab,ivocab,vec_output)\n",
    "print(f\"\\n\\n\\t\\tCosim score: {cosim(vec_output,target_label)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3e5f8-2a5f-4da8-9c96-30a4eb537c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4ec9a-1429-4caf-b3c2-01adc7a96a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
