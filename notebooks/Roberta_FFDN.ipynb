{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8355d28d-0e25-4c80-a559-6e4c0fb32010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae04ea98-80cf-46c6-82e5-d576b2e56729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"roberta-base\",output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "196d9351-6783-4686-bfe9-cc060a791bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'f', 'izer', 'Ġis', 'Ġworking', 'Ġwell']\n",
      "{'input_ids': [0, 642, 506, 6315, 16, 447, 157, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "test_sent = \"pfizer is working well\"\n",
    "print(tokenizer.tokenize(test_sent)) #check the tokenize\n",
    "\n",
    "print(tokenizer(test_sent))\n",
    "len_ids = len(tokenizer(test_sent)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1799af79-cedd-43ee-9b16-da0c38091282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'neum', 'onia']\n",
      "{'input_ids': [0, 642, 45042, 15402, 2], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "test_sent = \"pneumonia\"\n",
    "print(tokenizer.tokenize(test_sent)) #check the tokenize\n",
    "\n",
    "print(tokenizer(test_sent))\n",
    "len_ids = len(tokenizer(test_sent)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5cdf4-ed2b-497a-9161-cc1b654a5534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e4b2b38-a489-4262-ace4-2f29a3136e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: sosig_catto (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.12.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">sage-cosmos-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sosig_catto/Roberta%20SynNet%20Prototype\" target=\"_blank\">https://wandb.ai/sosig_catto/Roberta%20SynNet%20Prototype</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sosig_catto/Roberta%20SynNet%20Prototype/runs/ne9dd47w\" target=\"_blank\">https://wandb.ai/sosig_catto/Roberta%20SynNet%20Prototype/runs/ne9dd47w</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\hoang\\Desktop\\Light\\Torch_playground\\FYPv2\\notebooks\\wandb\\run-20210823_165732-ne9dd47w</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import wandb\n",
    "import re\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "wandb.init(project='Roberta SynNet Prototype')\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4bbc0b5-d121-4b5d-9765-77a8c86be985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseRobertaNet(\n",
      "  (act): ReLU()\n",
      "  (out): Tanh()\n",
      "  (hidden1): Linear(in_features=15360, out_features=2048, bias=True)\n",
      "  (hidden2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (hidden3): Linear(in_features=1024, out_features=768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DenseRobertaNet(nn.Module):\n",
    "    def __init__(self,context_length,embed_size=100):\n",
    "        super().__init__()\n",
    "        self.n = context_length*2\n",
    "        self.embed_size = embed_size\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Tanh() \n",
    "        self.hidden1 = nn.Linear(self.n*self.embed_size,2048)\n",
    "        self.hidden2 = nn.Linear(2048,1024)\n",
    "        self.hidden3 = nn.Linear(1024,self.embed_size)\n",
    " \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.act(self.hidden1(x))\n",
    "        x = self.act(self.hidden2(x))\n",
    "        x = self.out(self.hidden3(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "synmodel = DenseRobertaNet(context_length = 10,embed_size = 768)\n",
    "print(synmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8af8f893-54db-4bca-bf47-e5a660075aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "\n",
    "#Loading the data\n",
    "W_norm,vocab,ivocab = load_glove()\n",
    "    \n",
    "config.batch_size = 64\n",
    "\n",
    "training_files = '../processed_data/wiki_only/infection_corpus_stopwords_c10.txt' \n",
    "training_data = load_training_batch(training_files,config.batch_size)\n",
    "config.data = \"wiki_only_infection\"\n",
    "\n",
    "train_tensor = []\n",
    "for i,batch in enumerate(training_data):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    for sentence in batch:\n",
    "        y,x = sentence.split(':')\n",
    "        s = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "        data_list.append(s)\n",
    "        label_list.append(y)\n",
    "    train_tensor.append((data_list,label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23d4c91d-e1d1-4801-9e9f-3841b7f2864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = train_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad42021-e2ff-4963-baf5-0cc68925da41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01306c08-e59a-43da-917f-66dc268276d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roberta_embedding(word):  #data is list of string\n",
    "    ids = tokenizer(s)[\"input_ids\"]\n",
    "    len_ids = len(ids)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.tensor([ids]))\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_vecs_sum = []\n",
    "    for i in range(len_ids):\n",
    "        word_embed = hidden_states[-3][0][i] + hidden_states[-2][0][i] + hidden_states[-1][0][i]\n",
    "        token_vecs_sum.append(word_embed.detach().numpy())\n",
    "    word_vec = np.sum(token_vecs_sum,axis = 0)\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd12e9f8-ebcc-49bb-b85f-36b2e2aef150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "916c068f-ec89-4093-b36d-890af77612f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "config.lr = 0.0005\n",
    "config.momentum = 0.005\n",
    "optimizer = optim.SGD(model.parameters(),lr=config.lr,momentum=config.momentum,weight_decay=0.01)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def cosim(v1,v2):\n",
    "    return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, debug_set.shape[0], eta_min=config.lr)\n",
    "#learning rate adjustment -- try 0.001\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        data,y = batch\n",
    "        \n",
    "        features = []\n",
    "        labels = []\n",
    "        #get the embedding for the word as well as the sentence\n",
    "        for i,s in enumerate(data):\n",
    "            s_x = s.split(' ')\n",
    "            features.append([get_roberta_embedding(x) for x in s_x])\n",
    "            labels.append(get_roberta_embedding(y[i]))\n",
    "        #print(len(features),len(labels))\n",
    "        \n",
    "        feat = torch.Tensor(features).to(device)\n",
    "        label = torch.Tensor(labels).to(device)\n",
    "        \n",
    "        predictions = model(feat).squeeze(1)\n",
    "        loss = criterion(predictions,label)      \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        cosim_score = np.mean([cosim(labels[i],predictions[i].cpu().detach().numpy()) for i in range(config.batch_size) ])\n",
    "        \n",
    "    return epoch_loss, cosim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7587f82f-2e91-44a5-894b-abcbbdf9b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "debugsor = train_tensor[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "55593008-1664-462f-9518-ba0d9623bd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [07:44<07:44, 464.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:01\t|\tTrain Loss: 100.676\t|\tCosim score: -0.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [15:29<00:00, 464.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:02\t|\tTrain Loss: 101.019\t|\tCosim score: -0.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "wandb: Network error resolved after 0:00:38.062610, resuming normal operation.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "config.epochs = 2\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(config.epochs)):   \n",
    "    train_loss,score= train(synmodel.to(device),iter(debugsor), optimizer, criterion)\n",
    "\n",
    "    #epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    #if valid_loss < best_valid_loss:\n",
    "     #   best_valid_loss = valid_loss\n",
    "      #  torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    #print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    wandb.log({\"loss\":train_loss,\"cosim_score\":score})\n",
    "    print(f'Epoch:{epoch+1:02}\\t|\\tTrain Loss: {train_loss:.3f}\\t|\\tCosim score: {score:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c2766-cd2f-4930-989b-a296cbdb50c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367991bb-a574-4a32-9cf5-4537e256d58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
