{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631fdc74-92fe-4134-b21c-a7c1eaf480c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a7b14301e2ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pprint\n",
    "import time\n",
    "import argparse\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_google_search_page(input_text):\n",
    "    fox = webdriver.Firefox()\n",
    "    fox.get(\"https://www.google.com/\")\n",
    "    search_bar = fox.find_element_by_tag_name(\"input\")\n",
    "    search_bar.send_keys(input_text)\n",
    "    search_bar.send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    cur_url = fox.current_url\n",
    "    fox.close()\n",
    "    fox.quit()\n",
    "    return cur_url\n",
    "\n",
    "def get_top_results(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    all_urls = []\n",
    "    for elems in soup.findAll('h3'):\n",
    "        for link in soup.findAll('a'):\n",
    "            url = str(link.get('href'))\n",
    "            if url.startswith('/url?'):\n",
    "                all_urls.append(url[7:])\n",
    "    return all_urls\n",
    "\n",
    "def filter_duplicate(url_list,max_url=5):\n",
    "    domain_list = set()\n",
    "    result = dict()\n",
    "    blacklist = [\"youtube.com\"]\n",
    "    for url in url_list:\n",
    "        domain = re.findall(r'^https://www.(.*?)/',url) \n",
    "        url = re.findall(r'^https://(.*?)&sa',url) # &sa is there for the google rubbish\n",
    "        if domain != [] and domain[0] not in domain_list and domain[0] not in blacklist:   #it will not check[0] until domain != [] satisfy\n",
    "            domain_list.add('https://www.'+domain[0])\n",
    "            result['www.'+domain[0]] = \"https://\"+url[0]\n",
    "            if len(domain_list) == max_url:\n",
    "                break\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_elems_from_urls(urls):\n",
    "    stored_text = dict()\n",
    "    for url_ in urls.values():\n",
    "        #print(f'========{url_}===========')\n",
    "        try: \n",
    "            childpage = requests.get(url_)\n",
    "            childsoup = BeautifulSoup(childpage.content, 'html.parser')\n",
    "            stored_text[url_] = {'h1':set(),'p':set()}\n",
    "            for tagment in ['h1','p']:\n",
    "                    elements = childsoup.findAll(tagment)\n",
    "                    for element in elements:\n",
    "                        stored_text[url_][tagment].add(element.text)\n",
    "        except:\n",
    "            print(f\"ERROR at url {url_}\")\n",
    "            pass\n",
    "        #print(\"========================================================\")\n",
    "    return stored_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_sentence(sent):\n",
    "    sent = re.sub(f'[\\ \\r\\n]+',' ',sent)\n",
    "    return sent.lower()\n",
    "\n",
    "\n",
    "def write_text_dict_to_file(stored_text,outputfile='test.txt'):\n",
    "    with open(outputfile,'w',encoding='utf-8') as f:\n",
    "        for url,texts in stored_text.items():\n",
    "            f.write(f'=========={url}===========\\n')\n",
    "            clean_h1 = []\n",
    "            f.write(f'<<h1>>\\n')\n",
    "            for h1 in texts['h1']:\n",
    "                clean_h1 += sent_tokenize(h1)\n",
    "            for sentence in clean_h1:\n",
    "                f.write(clean_sentence(sentence))\n",
    "                f.write('\\n')\n",
    "            f.write(f'<<p>>\\n')\n",
    "            clean_p = []\n",
    "            for p in texts['p']:\n",
    "                clean_p += sent_tokenize(p)\n",
    "            for sentence in clean_p:\n",
    "                f.write(clean_sentence(sentence))\n",
    "                f.write('\\n')\n",
    "            f.write('\\n\\n')\n",
    "            print(f\"Finishing with url: {url}\")\n",
    "    print(\"Document ready\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"num_pages\", help=\"maximum number of pages crawled\",\n",
    "                        type=int)\n",
    "    parser.add_argument(\"input_text\", help=\"the query\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"output\", help=\"the query\",\n",
    "                        type=str)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    input_text = args.input_text\n",
    "\n",
    "    print(\"Getting the google page...\")\n",
    "    gg_url = get_google_search_page(input_text)\n",
    "    print(\"Get the first page results....\")\n",
    "    top_search = get_top_results(gg_url) \n",
    "    print(f\"Duplicate and filter to {args.num_pages} pages...\")\n",
    "    unique_top_search = filter_duplicate(top_search,args.num_pages)\n",
    "    print(\"Extracting the h1 and p elements from these pages...\")\n",
    "    stored_text = extract_text_elems_from_urls(unique_top_search)\n",
    "    print(\"Writing to file\")\n",
    "    write_text_dict_to_file(stored_text,args.output)\n",
    "    \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff3629-669b-42ab-82b6-d0eb8f01bf62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
