{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743c1190-2ee3-4e2d-823f-06faac1f2997",
   "metadata": {},
   "source": [
    "Similar to the GloVe, the pre-processing stage (section 1 and 2) is largely the same. Hence we will assume that there are pre-processed text corpus in the ./processed_data folder ready. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e328508c-7f27-4ac2-b816-c8494dad9b46",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s3\">Section 3: Training with Pytorch and Wandb</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88071ce-a645-46ef-a36b-05088571414d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: sosig_catto (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.12.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">true-puddle-45</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sosig_catto/Synthetic%20Net%20Roberta\" target=\"_blank\">https://wandb.ai/sosig_catto/Synthetic%20Net%20Roberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sosig_catto/Synthetic%20Net%20Roberta/runs/19lky0te\" target=\"_blank\">https://wandb.ai/sosig_catto/Synthetic%20Net%20Roberta/runs/19lky0te</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\hoang\\Desktop\\Light\\Torch_playground\\FYPv2\\demo\\wandb\\run-20211004_175000-19lky0te</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import wandb\n",
    "import re\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "wandb.init(project='Synthetic Net Roberta')\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71250a30-815e-444c-b52a-a2f7d8072bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (act): ReLU()\n",
      "  (out): Tanh()\n",
      "  (hidden1): Linear(in_features=15360, out_features=2048, bias=True)\n",
      "  (hidden2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (hidden3): Linear(in_features=512, out_features=768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,context_length,embed_size=100):\n",
    "        super().__init__()\n",
    "        self.n = context_length*2\n",
    "        self.embed_size = embed_size\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Tanh() \n",
    "        self.hidden1 = nn.Linear(self.n*self.embed_size,2048)\n",
    "        self.hidden2 = nn.Linear(2048,512)\n",
    "        self.hidden3 = nn.Linear(512,self.embed_size)\n",
    " \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.act(self.hidden1(x))\n",
    "        x = self.act(self.hidden2(x))\n",
    "        x = self.out(self.hidden3(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = DenseNet(context_length = 10,embed_size=768)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1ad435-a96a-4287-9192-3d7f8f9d2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#setup roberta\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch\n",
    "checkpoint='roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "roberta_model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e8b4f07-b415-4540-80f1-acda11a3b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.vocab\n",
    "roberta_embeddings = roberta_model.embeddings.word_embeddings.weight\n",
    "special_token_ids = tokenizer.convert_tokens_to_ids([tokenizer.pad_token,tokenizer.unk_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "285a341d-4c97-4c3f-ae86-211c50dc667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "\n",
    "#Loading the data\n",
    "    \n",
    "config.batch_size = 64\n",
    "\n",
    "target_words_list = ['tired'] # can be multiple words\n",
    "\n",
    "training_files = [f'./processed_data/{x}_corpus_c10.txt' for x in target_words_list]\n",
    "training_data = load_training_batch(training_files,config.batch_size)\n",
    "\n",
    "def token_to_id(word):\n",
    "    if word in vocab:\n",
    "        return vocab[word]\n",
    "    elif word == tokenizer.pad_token:\n",
    "        return special_token_ids[0]\n",
    "    else:\n",
    "        return special_token_ids[1]\n",
    "        \n",
    "def get_roberta_embedding(train_data):\n",
    "    train_tensor = []\n",
    "    for i,batch in enumerate(train_data):\n",
    "        batch_tensor_list = []\n",
    "        batch_label_list = []\n",
    "        for sentence in batch:\n",
    "            try:\n",
    "                y,x = sentence.split(':')\n",
    "                s = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "                input_ids = [token_to_id(word) for word in s.split(' ')]\n",
    "                context_embeddings = roberta_embeddings[input_ids].detach()\n",
    "                batch_tensor_list.append(context_embeddings)\n",
    "                y_id = token_to_id(y)\n",
    "                label_embedding = roberta_embeddings[y_id].detach()\n",
    "                batch_label_list.append(label_embedding)\n",
    "            except Exception as e:\n",
    "                print(sentence)\n",
    "                print(e)\n",
    "                return\n",
    "        train_tensor.append((batch_tensor_list,batch_label_list))\n",
    "    return train_tensor\n",
    "\n",
    "#list of tuples of tensors\n",
    "\n",
    "#for debug\n",
    "train_tensor = get_roberta_embedding(training_data[:1])\n",
    "#for training, remove the slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ee34e17-70ef-47bc-96ab-4f7e4710c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0a04ead-4e00-4ae9-bbc5-3f18285b9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper param\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "config.lr = 0.0005\n",
    "config.momentum = 0.005\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=config.lr,momentum=config.momentum,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def cosim(v1,v2):\n",
    "    return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, debug_set.shape[0], eta_min=config.lr)\n",
    "#learning rate adjustment -- try 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f788e796-e4b3-4204-842f-c1d9ed086a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:01\t|\tTrain Loss: 0.030\t|\tCosim score: -0.089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "config.epochs = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(config.epochs)):   \n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_cosim = 0\n",
    "    cosim_score = 0\n",
    "    model.train()\n",
    "  \n",
    "    for batch in iter(train_tensor):\n",
    "        optimizer.zero_grad()\n",
    "        features,labels = batch\n",
    "        feat = torch.stack(features)\n",
    "        label = torch.stack(labels)\n",
    "        \n",
    "        predictions = model(feat).squeeze(1)\n",
    "        loss = criterion(predictions,label)  \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        t_cosim_score = np.mean([cosim(label[i].detach().numpy(),predictions[i].detach().numpy()) for i in range(config.batch_size) ])\n",
    "        cosim_score += t_cosim_score\n",
    "    train_loss = epoch_loss\n",
    "    cosim_score = cosim_score/len(train_tensor)\n",
    "    wandb.log({\"loss\":train_loss,\"cosim_score\":cosim_score})\n",
    "    print(f'Epoch:{epoch+1:02}\\t|\\tTrain Loss: {train_loss:.3f}\\t|\\tCosim score: {cosim_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3149871-d07c-4c60-b4c6-5fb2a264c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),f'output/{date.today().strftime(\"%Y-%m\")}_{config.data}_{wandb.run.name}.pt')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217c3e3-9547-4d3a-a7db-705cfc4c6bf4",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s4\">Section 4: Testing</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aca700f4-0578-4e1b-a967-f4c514f86f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## section 4: Testing using analogy tests\n",
    "model_to_test =  DenseNet(context_length = 10,embed_size=768)\n",
    "model_to_test.load_state_dict(torch.load('./output/2021-10_roberta_capital_spring-dust-35.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13c98e53-1444-403f-8d35-fffbce370ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 -- \n",
      "\n",
      "\n",
      "\n",
      "                               Word       Unnormalized Cosine distance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "0                            capital\t\t1.000000\n",
      "1                            Capital\t\t0.999988\n",
      "2                           Ġcapital\t\t0.999946\n",
      "3                           ĠCapital\t\t0.999749\n",
      "4                          Ġcapitals\t\t0.998047\n",
      "5                         capitalist\t\t0.996167\n",
      "6                          resources\t\t0.994834\n",
      "7                          financial\t\t0.992938\n",
      "8                            liberal\t\t0.992446\n",
      "9                         industrial\t\t0.991878\n"
     ]
    }
   ],
   "source": [
    "file = './processed_data/pointless_corpus_c10.txt'\n",
    "with open(file,'r') as f:\n",
    "    infer_data = f.read().split('\\n')\n",
    "random_batch = [x for x in infer_data if x != '']\n",
    "random_batch = [x for x in infer_data if x != '']\n",
    "context_embeddings = []\n",
    "for sentence in random_batch:\n",
    "    y,x = sentence.split(':')\n",
    "    x = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "\n",
    "    input_ids = [token_to_id(word) for word in x.split(' ')]\n",
    "    context_embedding = roberta_embeddings[input_ids].detach()\n",
    "    context_embeddings.append(context_embedding)\n",
    "feat = torch.stack(context_embeddings)\n",
    "output = model_to_test(feat)\n",
    "vec_output = torch.mean(output,dim=0).detach().numpy().reshape(1,768)\n",
    "\n",
    "roberta_embedding_numpy = roberta_embeddings.detach().numpy()\n",
    "\n",
    "def __distance(W, vocab, ivocab, vec_output):\n",
    "\n",
    "    \n",
    "    dist = np.dot(W, vec_output.T).squeeze(1)\n",
    "    dist = 1/(1+np.exp(-dist))\n",
    "    a = np.argsort(-dist)[:10]\n",
    "\n",
    "    print(\"\\n                               Word       Unnormalized Cosine distance\\n\")\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    for i,x in enumerate(a):\n",
    "        print(\"%d%35s\\t\\t%f\" % (i,ivocab[x], dist[x]))\n",
    "print(f\"Test 1 -- \\n\\n\")\n",
    "\n",
    "__distance(roberta_embedding_numpy,vocab,{v:k for k,v in vocab.items()},vec_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ab23f-6705-4b1d-94d9-e20c39d560ad",
   "metadata": {},
   "source": [
    "The Inference Pipeline is similar to testing, with slight modification in the network as shown above. The files that are crawled to ./inference will work for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b30ae5-da26-46e6-aea5-2906bfe44fcd",
   "metadata": {},
   "source": [
    "### For modification of RoBERTa, please check the \"Roberta Modification Demo\" notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9556e50-99a6-47e1-b8a3-57e9a6a4b35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
