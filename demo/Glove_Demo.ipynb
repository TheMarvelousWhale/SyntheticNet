{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85175ec-58ec-4df7-b018-b9db6bdbc59d",
   "metadata": {},
   "source": [
    "## Embedding Synthesis Demo for GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9dc5e0-5aee-4c7f-815a-0804f2ac92ca",
   "metadata": {},
   "source": [
    "To make sure that you have everything needed, we will start from scratch for this notebook.  \\\n",
    "There is only one part that is not included (gathering wikitext -- or any other corpus) -- we assume it has been collected and is sitting on harddisk\\\n",
    "It is advised to run this notebook in a virtual environment \\\n",
    "Once you have done that, let's get the dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fec7b3-aa87-4b4d-91fb-5d82d0143626",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch tqdm wandb nltk selenium beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb0769-0063-4368-ad3b-05f8cdb599b0",
   "metadata": {},
   "source": [
    "_**Note**_: section 5 uses selenium web crawler and firefox, so it would be good if the browser is installed. Otherwise you would have to get the respective browser driver yourself (for e.g for chrome user it's [ChromeDriver](https://chromedriver.chromium.org/home)) and then modify the [function](#driver) here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4beea5f-504c-4627-be62-49c36873f88a",
   "metadata": {},
   "source": [
    "There are five sections to this notebook:\n",
    "* [Section 1: Preparing Corpus](#s1)\n",
    "* [Section 2: Preparing Training Data](#s2)\n",
    "* [Section 3: Training with Pytorch and Wandb](#s3)\n",
    "* [Section 4: Testing](#s4)\n",
    "* [Section 5: Inference](#s5)\n",
    "* [Section 6: Incorporate Synthetic Vector Into GloVe](#s6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0711e6-c2fd-49eb-90a7-3b4377792bc0",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s1\">Section 1: Preparing Corpus</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c538dce-b95d-44d2-a9b8-aef676355cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 862M/862M [03:39<00:00, 3.93MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
    "\n",
    "dest_file = './glove_6B.zip'\n",
    "# Streaming, so we can iterate over the response.\n",
    "response = requests.get(url, stream=True)\n",
    "total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "block_size = 1024 #1 Kibibyte\n",
    "progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "with open(dest_file, 'wb') as file:\n",
    "    for data in response.iter_content(block_size):\n",
    "        progress_bar.update(len(data))\n",
    "        file.write(data)\n",
    "progress_bar.close()\n",
    "if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "    print(\"ERROR, something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27c0d8e-9546-4627-a431-5afa62c85dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract to folder\n",
    "import zipfile\n",
    "with zipfile.ZipFile(dest_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dest_file.replace('.zip','') )  #use the filename as destination dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "565d5d05-2ed9-49e5-8019-f057df507631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the glove files\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def generate(file):\n",
    "    words = []\n",
    "    vectors = {}\n",
    "    with open(file, 'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            _temp = line.rstrip().split(' ')\n",
    "            words.append(_temp[0])\n",
    "            vectors[_temp[0]] = [float(x) for x in _temp[1:]]\n",
    "\n",
    "    vocab_size = len(words)\n",
    "    vocab = {w: idx for idx, w in enumerate(words)}\n",
    "    ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "\n",
    "    vector_dim = len(vectors[ivocab[0]])\n",
    "    W = np.zeros((vocab_size, vector_dim))\n",
    "    for word, v in vectors.items():\n",
    "        if word == '<unk>':\n",
    "            continue\n",
    "        W[vocab[word], :] = v\n",
    "\n",
    "    # normalize each word vector to unit variance\n",
    "    W_norm = np.zeros(W.shape)\n",
    "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "    W_norm = (W.T / d).T\n",
    "    return (W_norm, vocab, ivocab)\n",
    "\n",
    "glove_file = f\"{dest_file.replace('.zip','')}/glove.6B.100d.txt\"\n",
    "(W_norm, vocab, ivocab) = generate(glove_file)\n",
    "\n",
    "#save the files as npy for easier loading \n",
    "np.save('./glove6B100d.npy',W_norm)\n",
    "with open('./glove_vocab.json','w') as f:\n",
    "    json.dump(vocab,f)\n",
    "with open('./glove_ivocab.json','w') as f:\n",
    "    json.dump(ivocab,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f2aed-194f-475c-8855-07811db53188",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s2\">Section 2: Preparing Training Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bba83f4-0a73-471b-90fa-92fd1be2aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section covers the preparation pipeline. It is assumed that the wikitext is already downloaded and extracted\n",
    "# the wikipedia dump is here https://dumps.wikimedia.org/enwiki/20210920/enwiki-20210920-pages-articles-multistream.xml.bz2 \n",
    "# the source code to extract it is here https://github.com/attardi/wikiextractor \n",
    "\n",
    "\n",
    "#eventually, this part of the code just need the path to all the text files, \n",
    "#so it's up to you to implement it how you like\n",
    "import os, re\n",
    "wiki_dataset_dir = \"D:/DATASETS_UNZIP/Datasets/text\"\n",
    "#Extracting from wiki \n",
    "filepath_list = []\n",
    "wiki=True\n",
    "if wiki == True:\n",
    "    for folder in os.listdir(wiki_dataset_dir):\n",
    "        for file in os.listdir(wiki_dataset_dir+'/'+folder):\n",
    "            filepath_list.append(wiki_dataset_dir+'/'+folder+'/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d085e2a-02c4-444a-905c-88cc0e82b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279e5b3-9b04-4f52-bfa1-30a2a159f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this if you dont have nltk stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a8b2b-60aa-472c-9822-69976b34a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b606f234-b224-47e1-a297-a9e233d55d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the target word and neighboring word from its sentences\n",
    "\n",
    "def extract(sentence,target_word,context_length,pad,debug=True):\n",
    "    target_word = target_word.lower()\n",
    "    sentence = sentence.lower()\n",
    "    if target_word not in sentence: #reduce processing\n",
    "        return\n",
    "    \n",
    "    s = sentence.lower().strip()\n",
    "    s = re.sub('[\\n\\r\\ ]+',' ',s)\n",
    "    s = re.sub('[^a-z ]+','',s)\n",
    "    \n",
    "    t = target_word\n",
    "    raw_tokens = s.split(' ')\n",
    "    tokens = [i for i in raw_tokens if i != '' and i not in stopword_list]\n",
    "    word_list = []\n",
    "    if t in tokens:\n",
    "        __index = tokens.index(t)        # this only get one utterance, what about other utterances? \n",
    "        if __index < context_length:      #pad front\n",
    "            word_list += [pad for _ in range(context_length-__index)]\n",
    "            word_list += tokens[:__index]      #pad back\n",
    "        else:\n",
    "            word_list += tokens[__index-context_length:__index]\n",
    "        if __index + context_length >= len(tokens):\n",
    "            word_list += tokens[__index+1:]\n",
    "            word_list += [pad for _ in range(context_length + __index + 1 - len(tokens))]\n",
    "        else:\n",
    "            word_list += tokens[__index+1:__index+context_length+1]\n",
    "        return word_list\n",
    "\n",
    "\n",
    "    else:   #target not found:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d93340ad-0721-4b42-a680-687747c1c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda25d1-6547-4957-885b-70c189587c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawling multiple words at once, writing the file into target directory  \n",
    "\n",
    "target_words = ['tired','pointless']\n",
    "                \n",
    "context_length = 10\n",
    "\n",
    "for target_word in target_words:\n",
    "    line_written = 0\n",
    "    corpus = f\"./processed_data/{target_word}_corpus_c{context_length}.txt\"\n",
    "    with open(corpus,'a') as o10:\n",
    "        for file in tqdm(filepath_list):\n",
    "            with open(file,'r',encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    result = extract(line,target_word,context_length,\"<pad>\",True)\n",
    "                    if result == None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        o10.write(f'{target_word}:')\n",
    "                        o10.write(''.join([i+' ' for i in result] ))\n",
    "                        o10.write('\\n')\n",
    "                        line_written += 1\n",
    "                        if line_written % 10000 == 0:\n",
    "                            print(f\"Written {line_written} into file {target_word}\")\n",
    "        print(\"\\n\\n DONE WITH CORPUS {corpus}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58115d9a-b724-4fe7-9c59-22ffb0ef1184",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s3\">Section 3: Training with Pytorch and Wandb</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "787a3173-be67-4b96-99e3-d20ab5955052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: sosig_catto (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.12.2 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cool-hill-39</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sosig_catto/Synthetic%20Net\" target=\"_blank\">https://wandb.ai/sosig_catto/Synthetic%20Net</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sosig_catto/Synthetic%20Net/runs/1887l11b\" target=\"_blank\">https://wandb.ai/sosig_catto/Synthetic%20Net/runs/1887l11b</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\hoang\\Desktop\\Light\\Torch_playground\\FYPv2\\demo\\wandb\\run-20210927_174629-1887l11b</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "wandb.init(project='Synthetic Net')\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36c9c146-6b8b-45c3-8280-2cbb31f9ece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (act): ReLU()\n",
      "  (out): Tanh()\n",
      "  (hidden1): Linear(in_features=2000, out_features=2048, bias=True)\n",
      "  (hidden2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (hidden3): Linear(in_features=512, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,context_length,embed_size=100):\n",
    "        super().__init__()\n",
    "        self.n = context_length*2\n",
    "        self.embed_size = 100\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Tanh() \n",
    "        self.hidden1 = nn.Linear(self.n*self.embed_size,2048)\n",
    "        self.hidden2 = nn.Linear(2048,512)\n",
    "        self.hidden3 = nn.Linear(512,self.embed_size)\n",
    " \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.act(self.hidden1(x))\n",
    "        x = self.act(self.hidden2(x))\n",
    "        x = self.out(self.hidden3(x))\n",
    "        return x\n",
    "    \n",
    "config.context_length = 10\n",
    "model = DenseNet(context_length = config.context_length)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "614d433d-ea17-46b8-89b6-87092105bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#util file contains the loading functions\n",
    "from util import *\n",
    "\n",
    "#Loading the data\n",
    "W_norm,vocab,ivocab = load_glove(        \n",
    "        weight_file = './glove6B100d.npy',\n",
    "        vocab_file = './glove_vocab.json',\n",
    "        ivocab_file='./glove_ivocab.json'\n",
    ")\n",
    "    \n",
    "config.batch_size = 64\n",
    "\n",
    "#configure the files used for training. Can load multiple files\n",
    "#usually load with negative samples to avoid overfitting\n",
    "files_for_training =['tired'] \n",
    "\n",
    "training_files = [f'./processed_data/{x}_corpus_c10.txt' for x in files_for_training]\n",
    "\n",
    "training_data = load_training_batch(training_files,config.batch_size)\n",
    "\n",
    "#for logging purpose, provide data lineage\n",
    "config.data = \"wiki_only\"\n",
    "\n",
    "train_tensor = get_embedding(training_data,W_norm,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9767b1e4-9a34-44fd-a8c6-03dd95382948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "### checking vocab\n",
    "print(len(vocab))\n",
    "print(len(ivocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7124e79a-c7ea-48ad-b90b-30aa3cc237d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "config.lr = 0.0005\n",
    "config.momentum = 0.005\n",
    "optimizer = optim.SGD(model.parameters(),lr=config.lr,momentum=config.momentum,weight_decay=0.01)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def cosim(v1,v2):\n",
    "    return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, debug_set.shape[0], eta_min=config.lr)\n",
    "#learning rate adjustment -- try 0.001\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        features,labels = batch\n",
    "        batch_size = features.shape[0]\n",
    "        predictions = model(torch.Tensor(features)).squeeze(1)\n",
    "        loss = criterion(predictions,torch.Tensor(labels))      \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        cosim_score = np.mean([cosim(labels[i],predictions[i].detach().numpy()) for i in range(batch_size) ])\n",
    "        \n",
    "    return epoch_loss,cosim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "214fcfe4-081e-4a90-9422-51b97ea7af0a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                                 | 1/40 [00:07<04:48,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:01\t|\tTrain Loss: 18.281\t|\tCosim score: 0.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 2/40 [00:14<04:43,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:02\t|\tTrain Loss: 17.854\t|\tCosim score: 0.194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▏                                                                            | 3/40 [00:22<04:37,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:03\t|\tTrain Loss: 17.436\t|\tCosim score: 0.249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 4/40 [00:30<04:35,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:04\t|\tTrain Loss: 17.026\t|\tCosim score: 0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▍                                                                        | 5/40 [00:38<04:32,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:05\t|\tTrain Loss: 16.624\t|\tCosim score: 0.352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▍                                                                      | 6/40 [00:46<04:28,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:06\t|\tTrain Loss: 16.229\t|\tCosim score: 0.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████▌                                                                    | 7/40 [00:54<04:22,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:07\t|\tTrain Loss: 15.840\t|\tCosim score: 0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 8/40 [01:02<04:17,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:08\t|\tTrain Loss: 15.458\t|\tCosim score: 0.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████▋                                                                | 9/40 [01:10<04:10,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:09\t|\tTrain Loss: 15.083\t|\tCosim score: 0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████▌                                                             | 10/40 [01:19<04:04,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10\t|\tTrain Loss: 14.713\t|\tCosim score: 0.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▌                                                           | 11/40 [01:27<03:55,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11\t|\tTrain Loss: 14.349\t|\tCosim score: 0.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▌                                                         | 12/40 [01:35<03:48,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:12\t|\tTrain Loss: 13.992\t|\tCosim score: 0.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▋                                                       | 13/40 [01:43<03:38,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:13\t|\tTrain Loss: 13.640\t|\tCosim score: 0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████████████████████▋                                                     | 14/40 [01:51<03:30,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:14\t|\tTrain Loss: 13.294\t|\tCosim score: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████▊                                                   | 15/40 [01:59<03:22,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:15\t|\tTrain Loss: 12.955\t|\tCosim score: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▊                                                 | 16/40 [02:08<03:15,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:16\t|\tTrain Loss: 12.622\t|\tCosim score: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████▊                                               | 17/40 [02:16<03:07,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:17\t|\tTrain Loss: 12.295\t|\tCosim score: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████▉                                             | 18/40 [02:24<02:59,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:18\t|\tTrain Loss: 11.975\t|\tCosim score: 0.749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████▉                                           | 19/40 [02:32<02:51,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:19\t|\tTrain Loss: 11.661\t|\tCosim score: 0.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████                                         | 20/40 [02:40<02:42,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20\t|\tTrain Loss: 11.354\t|\tCosim score: 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████                                       | 21/40 [02:48<02:34,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:21\t|\tTrain Loss: 11.053\t|\tCosim score: 0.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████                                     | 22/40 [02:56<02:27,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:22\t|\tTrain Loss: 10.760\t|\tCosim score: 0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████████████▏                                  | 23/40 [03:05<02:18,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:23\t|\tTrain Loss: 10.473\t|\tCosim score: 0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▏                                | 24/40 [03:13<02:12,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:24\t|\tTrain Loss: 10.192\t|\tCosim score: 0.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████▎                              | 25/40 [03:22<02:04,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:25\t|\tTrain Loss: 9.918\t|\tCosim score: 0.832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████████████████████████████████▎                            | 26/40 [03:30<01:56,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:26\t|\tTrain Loss: 9.651\t|\tCosim score: 0.841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████████████████████████████████████▎                          | 27/40 [03:40<01:55,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:27\t|\tTrain Loss: 9.391\t|\tCosim score: 0.849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████▍                        | 28/40 [03:49<01:46,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:28\t|\tTrain Loss: 9.138\t|\tCosim score: 0.856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████▍                      | 29/40 [03:58<01:37,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:29\t|\tTrain Loss: 8.893\t|\tCosim score: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████▌                    | 30/40 [04:06<01:27,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:30\t|\tTrain Loss: 8.655\t|\tCosim score: 0.870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████▌                  | 31/40 [04:15<01:18,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31\t|\tTrain Loss: 8.424\t|\tCosim score: 0.876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 32/40 [04:24<01:09,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:32\t|\tTrain Loss: 8.202\t|\tCosim score: 0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████████▋              | 33/40 [04:32<01:00,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:33\t|\tTrain Loss: 7.987\t|\tCosim score: 0.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 34/40 [04:40<00:50,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:34\t|\tTrain Loss: 7.779\t|\tCosim score: 0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 35/40 [04:49<00:42,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:35\t|\tTrain Loss: 7.579\t|\tCosim score: 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 36/40 [04:57<00:33,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:36\t|\tTrain Loss: 7.386\t|\tCosim score: 0.902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████▊      | 37/40 [05:05<00:24,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:37\t|\tTrain Loss: 7.199\t|\tCosim score: 0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 38/40 [05:13<00:16,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:38\t|\tTrain Loss: 7.019\t|\tCosim score: 0.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████████████████████████████████████▉  | 39/40 [05:21<00:08,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:39\t|\tTrain Loss: 6.844\t|\tCosim score: 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [05:29<00:00,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40\t|\tTrain Loss: 6.676\t|\tCosim score: 0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config.epochs = 40   #usually 40 is the best\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(config.epochs)):   \n",
    "    train_loss,cosim_score= train(model,iter(train_tensor), optimizer, criterion)\n",
    "\n",
    "    #epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    #if valid_loss < best_valid_loss:\n",
    "     #   best_valid_loss = valid_loss\n",
    "      #  torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    #print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    wandb.log({\"loss\":train_loss,\"cosim_score\":cosim_score})\n",
    "    print(f'Epoch:{epoch+1:02}\\t|\\tTrain Loss: {train_loss:.3f}\\t|\\tCosim score: {cosim_score:.3f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31393ac2-be1f-4622-bed1-a3a43ce8f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir output\n",
    "torch.save(model.state_dict(),f'output/{date.today().strftime(\"%Y-%m\")}_{config.data}_{wandb.run.name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261e270a-ce19-4a11-84f3-75931f23852a",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s4\">Section 4: Testing</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46ab9c27-8f33-4f32-947f-052f6e74ef68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "Test 1 -- sample sentence: \n",
      "\n",
      "tired:short run run sweet road runner real scored music ten feathered clippety clobbered used set generic musical cues follow action \n",
      "\n",
      "\n",
      "\n",
      "(400000,)\n",
      "\n",
      "                               Word       Unnormalized Cosine distance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "0                    multilateralism\t\t0.190550\n",
      "1                          resurging\t\t0.161833\n",
      "2                        replicators\t\t0.156357\n",
      "3                       undiminished\t\t0.147284\n",
      "4                            sinning\t\t0.145458\n",
      "5                            finning\t\t0.143951\n",
      "6                             glatch\t\t0.143233\n",
      "7                      neoliberalism\t\t0.142868\n",
      "8                      americanizing\t\t0.142485\n",
      "9                          labarbera\t\t0.142431\n",
      "\n",
      "\n",
      "\t\tCosim score: [0.09054023]\n"
     ]
    }
   ],
   "source": [
    "## section 4: Testing using analogy tests\n",
    "model_to_test =  DenseNet(context_length = 10)\n",
    "model_to_test.load_state_dict(torch.load('./output/2021-09_wiki_only_cool-hill-39.pt'))\n",
    "\n",
    "##Testing has 3 unit tests\n",
    "#Test 1 sentence\n",
    "#Test 1 batch\n",
    "#custom sentence\n",
    "\n",
    "\n",
    "#Test 1 -- random sentence in training\n",
    "random_sent = random.choice(training_data)[random.randint(0,config.batch_size-1)]\n",
    "y,x = random_sent.split(':')\n",
    "x = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "sample_tensor = torch.Tensor([[get_glove_vec(word,W_norm,vocab) for word in x.split(' ')]])\n",
    "sample_output = model_to_test(sample_tensor)\n",
    "target_label = np.array(get_glove_vec(y,W_norm,vocab))\n",
    "\n",
    "output1 = sample_output.squeeze(1)\n",
    "vec_output1 = output.detach().numpy()\n",
    "print(vec_output1.shape)\n",
    "\n",
    "def __distance(W, vocab, ivocab, vec_output):\n",
    "\n",
    "\n",
    "    dist = np.dot(W, vec_output.T).squeeze(1)\n",
    "    print(dist.shape)\n",
    "    a = np.argsort(-dist)[:10]\n",
    "\n",
    "    print(\"\\n                               Word       Unnormalized Cosine distance\\n\")\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    for i,x in enumerate(a):\n",
    "        print(\"%d%35s\\t\\t%f\" % (i,ivocab[str(x)], dist[x]))\n",
    "print(f\"Test 1 -- sample sentence: \\n\\n{random_sent}\\n\\n\")\n",
    "\n",
    "__distance(W_norm,vocab,ivocab,vec_output1)\n",
    "\n",
    "\n",
    "print(f\"\\n\\n\\t\\tCosim score: {cosim(vec_output1,target_label)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3caef383-4354-4190-bcc6-8ed52d9c76af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2 -- sample batch: \n",
      "\n",
      "\n",
      "(400000,)\n",
      "\n",
      "                               Word       Unnormalized Cosine distance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "0                              tired\t\t0.596813\n",
      "1                             scared\t\t0.462481\n",
      "2                              weary\t\t0.460485\n",
      "3                                 'm\t\t0.446562\n",
      "4                         frustrated\t\t0.442676\n",
      "5                           fatigued\t\t0.438139\n",
      "6                          exhausted\t\t0.436740\n",
      "7                              bored\t\t0.434672\n",
      "8                             afraid\t\t0.433339\n",
      "9                               feel\t\t0.431909\n",
      "\n",
      "\n",
      "\t\tCosim score: [0.14825795]\n"
     ]
    }
   ],
   "source": [
    "#test 2: test by batch\n",
    "\n",
    "random_batch = random.choice(training_data)\n",
    "sample_batch_tensor = []\n",
    "target_batch_tensor = []\n",
    "for sentence in random_batch:\n",
    "    y,x = sentence.split(':')\n",
    "    x = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "    sample_tensor = [get_glove_vec(word,W_norm,vocab) for word in x.split(' ')]\n",
    "    target_batch_tensor.append(get_glove_vec(y,W_norm,vocab))\n",
    "    sample_batch_tensor.append(sample_tensor)\n",
    "    \n",
    "sample_batch_tensor = torch.Tensor(np.array(sample_batch_tensor))\n",
    "target_batch_tensor = np.array(target_batch_tensor)\n",
    "\n",
    "sample_output = model(sample_batch_tensor)\n",
    "\n",
    "output2 = torch.mean(sample_output,0)   #sum across embeddings\n",
    "vec_output2 = output2.detach().numpy().reshape((1,100))\n",
    "\n",
    "print(f\"Test 2 -- sample batch: \\n\\n\")\n",
    "\n",
    "__distance(W_norm,vocab,ivocab,vec_output2)\n",
    "\n",
    "\n",
    "print(f\"\\n\\n\\t\\tCosim score: {cosim(vec_output2,target_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa79a25f-c409-4d12-ac6e-2bdebdb494a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3: Custom Test\n",
      "\n",
      "pacific disaster response fund support armenian government fight spread covid year bank committed million loan electric networks armenia ensure electricity\n",
      "\n",
      "\n",
      "(400000,)\n",
      "\n",
      "                               Word       Unnormalized Cosine distance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "0                    multilateralism\t\t0.190550\n",
      "1                          resurging\t\t0.161833\n",
      "2                        replicators\t\t0.156357\n",
      "3                       undiminished\t\t0.147284\n",
      "4                            sinning\t\t0.145458\n",
      "5                            finning\t\t0.143951\n",
      "6                             glatch\t\t0.143233\n",
      "7                      neoliberalism\t\t0.142868\n",
      "8                      americanizing\t\t0.142485\n",
      "9                          labarbera\t\t0.142431\n",
      "\n",
      "\n",
      "\t\tCosim score: [0.12973318]\n"
     ]
    }
   ],
   "source": [
    "###Test 3: Custom\n",
    "\n",
    "\n",
    "random_sent = 'pacific disaster response fund support armenian government fight spread covid year bank committed million loan electric networks armenia ensure electricity '\n",
    "target_word = 'pneumonia'\n",
    "target_label = np.array(get_glove_vec(target_word,W_norm,vocab))\n",
    "random_sent = re.sub('[\\n\\r\\ ]+',' ',random_sent).strip()\n",
    "\n",
    "sample_tensor = torch.Tensor([[get_glove_vec(word,W_norm,vocab) for word in random_sent.split(' ')]])\n",
    "sample_output = model(sample_tensor)\n",
    "output3 = sample_output.squeeze(1)\n",
    "vec_output3 = output3.detach().numpy()\n",
    "print(f\"Test 3: Custom Test\\n\\n{random_sent}\\n\\n\")\n",
    "__distance(W_norm,vocab,ivocab,vec_output3)\n",
    "print(f\"\\n\\n\\t\\tCosim score: {cosim(vec_output3,target_label)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9cba6-5f8d-4b85-91b1-02143a6a347e",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s5\">Section 5: Inference</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac467452-f105-4d95-b330-6656ad92ade7",
   "metadata": {},
   "source": [
    "**Note**: the driver function is <a class=\"anchor\" id=\"driver\">get_google_search_page</a>. The driver is currently defined as Firefox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "82d3cfce-928f-4af4-a200-304993637f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the google page...\n",
      "Get the first page results....\n",
      "Duplicate and filter to 100 pages...\n",
      "Extracting the h1 and p elements from these pages...\n",
      "Writing to file\n",
      "Finishing with url: https://www.bbc.com/news/magazine-33809914\n",
      "Finishing with url: https://www.lamborghini.com/en-en/models/limited-series/sian-fkp-37\n",
      "Finishing with url: https://www.lego.com/en-sg/product/lamborghini-sian-fkp-37-42115\n",
      "Finishing with url: https://www.quora.com/What-does-Sian-Singlish-mean-in-English\n",
      "Finishing with url: https://www.nortonrosefulbright.com/en-sg/people/123129\n",
      "Finishing with url: https://www.autoexpress.co.uk/lamborghini/sian\n",
      "Document ready\n"
     ]
    }
   ],
   "source": [
    "## section 5: generating for an unknown word \n",
    "\n",
    "#we will crawl some data for an unknown words using selenium, and export it to a text file\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pprint\n",
    "import time\n",
    "import argparse\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_google_search_page(input_text):\n",
    "    fox = webdriver.Firefox()\n",
    "    fox.get(f\"https://www.google.com/search?q={input_text}\")\n",
    "    search_bar = fox.find_element_by_tag_name(\"input\")\n",
    "    time.sleep(2)\n",
    "    cur_url = fox.current_url\n",
    "    fox.close()\n",
    "    fox.quit()\n",
    "    return cur_url\n",
    "\n",
    "def get_top_results(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    all_urls = []\n",
    "    for elems in soup.findAll('h3'):\n",
    "        for link in soup.findAll('a'):\n",
    "            url = str(link.get('href'))\n",
    "            if url.startswith('/url?'):\n",
    "                all_urls.append(url[7:])\n",
    "    return all_urls\n",
    "\n",
    "def filter_duplicate(url_list,max_url=5):\n",
    "    domain_list = set()\n",
    "    result = dict()\n",
    "    blacklist = [\"youtube.com\"]\n",
    "    for url in url_list:\n",
    "        domain = re.findall(r'^https://www.(.*?)/',url) \n",
    "        url = re.findall(r'^https://(.*?)&sa',url) # &sa is there for the google rubbish\n",
    "        if domain != [] and domain[0] not in domain_list and domain[0] not in blacklist:   #it will not check[0] until domain != [] satisfy\n",
    "            domain_list.add('https://www.'+domain[0])\n",
    "            result['www.'+domain[0]] = \"https://\"+url[0]\n",
    "            if len(domain_list) == max_url:\n",
    "                break\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_elems_from_urls(urls):\n",
    "    stored_text = dict()\n",
    "    for url_ in urls.values():\n",
    "        #print(f'========{url_}===========')\n",
    "        try: \n",
    "            childpage = requests.get(url_)\n",
    "            childsoup = BeautifulSoup(childpage.content, 'html.parser')\n",
    "            stored_text[url_] = {'h1':set(),'p':set()}\n",
    "            for tagment in ['h1','p']:\n",
    "                    elements = childsoup.findAll(tagment)\n",
    "                    for element in elements:\n",
    "                        stored_text[url_][tagment].add(element.text)\n",
    "        except:\n",
    "            print(f\"ERROR at url {url_}\")\n",
    "            pass\n",
    "        #print(\"========================================================\")\n",
    "    return stored_text\n",
    "\n",
    "\n",
    "def clean_sentence(sent):\n",
    "    sent = re.sub(f'[\\ \\r\\n]+',' ',sent)\n",
    "    return sent.lower()\n",
    "\n",
    "\n",
    "def write_text_dict_to_file(stored_text,outputfile='test.txt'):\n",
    "    with open(outputfile,'w',encoding='utf-8') as f:\n",
    "        for url,texts in stored_text.items():\n",
    "            f.write(f'=========={url}===========\\n')\n",
    "            clean_h1 = []\n",
    "            f.write(f'<<h1>>\\n')\n",
    "            for h1 in texts['h1']:\n",
    "                clean_h1 += sent_tokenize(h1)\n",
    "            for sentence in clean_h1:\n",
    "                f.write(clean_sentence(sentence))\n",
    "                f.write('\\n')\n",
    "            f.write(f'<<p>>\\n')\n",
    "            clean_p = []\n",
    "            for p in texts['p']:\n",
    "                clean_p += sent_tokenize(p)\n",
    "            for sentence in clean_p:\n",
    "                f.write(clean_sentence(sentence))\n",
    "                f.write('\\n')\n",
    "            f.write('\\n\\n')\n",
    "            print(f\"Finishing with url: {url}\")\n",
    "    print(\"Document ready\")\n",
    "\n",
    "\n",
    "def main(input_text,output,num_pages=10):\n",
    "    print(\"Getting the google page...\")\n",
    "    gg_url = get_google_search_page(input_text)\n",
    "    print(\"Get the first page results....\")\n",
    "    top_search = get_top_results(gg_url) \n",
    "    print(f\"Duplicate and filter to {num_pages} pages...\")\n",
    "    unique_top_search = filter_duplicate(top_search,num_pages)\n",
    "    print(\"Extracting the h1 and p elements from these pages...\")\n",
    "    stored_text = extract_text_elems_from_urls(unique_top_search)\n",
    "    print(\"Writing to file\")\n",
    "    write_text_dict_to_file(stored_text,output)\n",
    "    \n",
    "main('sian','./inference/sian.txt',num_pages = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9cc016c7-a15b-4624-8604-327e8e0c36fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 334/334 [00:00<00:00, 27829.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['among', 'students', 'include', 'mugging', 'means', 'studying', 'hard', 'siao', 'meaning', 'extreme', 'meaning', 'tired', 'boring', 'vanessa', 'kin', 'singapore', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'one', 'word', 'singlish', 'used', 'many', 'contexts', 'accurately', 'expresses', 'emotions', 'like']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'singlish', 'everyday', 'go', 'work', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'singlish', 'plane', 'delay', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'new', 'lamborghini', 'review', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', 'beneath', 'dramatic', 'new', 'carbonfibre', 'skin', 'styled', 'internally', 'lamborghinis', 'designers', 'almost', 'exactly', 'aventador', 'svj', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'slightly', 'cynical', 'car', 'made', 'lamborghini', 'small', 'fortune', 'process', 'despite', 'costing']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'gearchanges', 'also', 'quite', 'slow', 'keep', 'default', 'startup', 'mode', 'strada', 'street', 'english', 'leaves', 'transmission', 'auto', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'mind', 'begin', 'feel', 'like', 'might', 'worth', 'outrageous', 'amount', 'money', 'costs', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'matter', 'one', 'cars', 'rest', 'us', 'ever', 'aspire', 'photographs', 'occasionally', 'get', 'glimpse']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'move', 'drives', 'lot', 'like', 'looks', 'say', 'dramatically', 'extreme', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'cant', 'buy', 'lamborghini', 'fkp', 'examples', 'sold', 'minute', 'went', 'sale', 'frankfurt', 'motor', 'show', 'remember']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#cleaning the crawled text for inference\n",
    "# cleaning\n",
    "inference_target = 'sian'\n",
    "inference_file = f\"./inference/{inference_target}.txt\"\n",
    "inference_output_file = f\"./inference/clean_{inference_target}.txt\"\n",
    "with open(inference_output_file,'w') as o:\n",
    "    with open(inference_file,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            result = extract(line,inference_target,10,\"<pad>\",True)\n",
    "            if result == None:\n",
    "                continue\n",
    "            else:\n",
    "                print(result)\n",
    "                o.write(f'{inference_target}:')\n",
    "                o.write(''.join([i+' ' for i in result] ))\n",
    "                o.write('\\n')\n",
    "                line_written += 1\n",
    "                if line_written % 10000 == 0:\n",
    "                    print(f\"Written {line_written} into file {inference_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61d61677-8180-4f4f-b8ba-0712ff1acd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference batch: sian\n",
      "\n",
      "\n",
      "(400000,)\n",
      "\n",
      "                               Word       Unnormalized Cosine distance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "0                              tired\t\t0.531570\n",
      "1                             scared\t\t0.414386\n",
      "2                              weary\t\t0.412458\n",
      "3                                 'm\t\t0.400601\n",
      "4                         frustrated\t\t0.399057\n",
      "5                          exhausted\t\t0.392542\n",
      "6                             afraid\t\t0.389355\n",
      "7                               feel\t\t0.388964\n",
      "8                              bored\t\t0.386259\n",
      "9                           fatigued\t\t0.386248\n",
      "\n",
      "\n",
      "\t\tCosim score: [0.15159323]\n"
     ]
    }
   ],
   "source": [
    "with open(inference_output_file,'r') as f:\n",
    "    batch = f.readlines()\n",
    "    \n",
    "sample_batch_tensor = []\n",
    "target_batch_tensor = []\n",
    "for sentence in batch:\n",
    "    y,x = sentence.split(':')\n",
    "    x = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "    sample_tensor = [get_glove_vec(word,W_norm,vocab) for word in x.split(' ')]\n",
    "    target_batch_tensor.append(get_glove_vec(y,W_norm,vocab))\n",
    "    sample_batch_tensor.append(sample_tensor)\n",
    "    \n",
    "sample_batch_tensor = torch.Tensor(np.array(sample_batch_tensor))\n",
    "target_batch_tensor = np.array(target_batch_tensor)\n",
    "\n",
    "sample_output = model(sample_batch_tensor)\n",
    "\n",
    "output = torch.mean(sample_output,0)   #sum across embeddings\n",
    "infer_vec_output = output.detach().numpy().reshape((1,100))\n",
    "\n",
    "print(f\"Inference batch: {inference_target}\\n\\n\")\n",
    "\n",
    "__distance(W_norm,vocab,ivocab,infer_vec_output)\n",
    "\n",
    "\n",
    "print(f\"\\n\\n\\t\\tCosim score: {cosim(infer_vec_output,target_label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c796e-aa08-4f5d-9553-a51654f7a3b7",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s6\">Section 6: Incorporating synthesis vector back into GloVe</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "fa8f86c7-32e4-4733-870f-552f4f23da80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "400000\n",
      "['sian'] []\n"
     ]
    }
   ],
   "source": [
    "## section 6: writing the vectors back \n",
    "\n",
    "\n",
    "#choose an output vec that performs well \n",
    "output_vec = infer_vec_output\n",
    "embeddings = {inference_target:output_vec}  #word: embedding\n",
    "\n",
    "original_glove = './glove_6B/glove.6B.100d.txt'\n",
    "modified_glove = './glove_6B/modded_glove.6B.100d.txt'\n",
    "\n",
    "#the word might have existed in the corpus, so we will play safe\n",
    "existing = []\n",
    "with open(modified_glove,'w',encoding='utf-8') as f:\n",
    "    with open(original_glove,'r',encoding='utf-8') as i:\n",
    "        data = i.readlines()\n",
    "        for line in data:\n",
    "            word = line.rstrip().split(' ')[0]\n",
    "            if word in embeddings:  \n",
    "                f.write(f\"{word} \")  \n",
    "                f.write(' '.join([f'{i:.5f}' for i in embeddings[word].flatten().tolist()]))\n",
    "                f.write('\\n')\n",
    "                existing.append(word)\n",
    "            else:\n",
    "                f.write(line)\n",
    "        non_existing = [i for i in embeddings.keys() if i not in existing]\n",
    "        for new_word in non_existing:\n",
    "            f.write(f\"{word} \")  \n",
    "            f.write(' '.join([f'{i:.5f}' for i in embeddings[word].flatten().tolist()]))\n",
    "            f.write('\\n')\n",
    "\n",
    "#checking if it has been incorporated\n",
    "with open(modified_glove,'r',encoding='utf-8') as f:\n",
    "    temp = f.readlines()\n",
    "print(len(temp))\n",
    "(m_W_norm, mod_vocab, m_ivocab) = generate(modified_glove)\n",
    "print(len(mod_vocab))\n",
    "print(existing,non_existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4e57d-e2cf-47a1-b29e-a93a1f0b4620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
