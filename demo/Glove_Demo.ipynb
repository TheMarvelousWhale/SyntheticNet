{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85175ec-58ec-4df7-b018-b9db6bdbc59d",
   "metadata": {},
   "source": [
    "## Embedding Synthesis Demo for GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9dc5e0-5aee-4c7f-815a-0804f2ac92ca",
   "metadata": {},
   "source": [
    "To make sure that you have everything needed, we will start from scratch for this notebook.  \\\n",
    "There is only one part that is not included (gathering wikitext -- or any other corpus) -- we assume it has been collected and is sitting on harddisk\\\n",
    "It is advised to run this notebook in a virtual environment \\\n",
    "Once you have done that, let's get the dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fec7b3-aa87-4b4d-91fb-5d82d0143626",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch tqdm wandb nltk selenium beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb0769-0063-4368-ad3b-05f8cdb599b0",
   "metadata": {},
   "source": [
    "_**Note**_: section 5 uses selenium web crawler and firefox, so it would be good if the browser is installed. Otherwise you would have to get the respective browser driver yourself (for e.g for chrome user it's [ChromeDriver](https://chromedriver.chromium.org/home)) and then modify the [function](#driver) here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4beea5f-504c-4627-be62-49c36873f88a",
   "metadata": {},
   "source": [
    "There are five sections to this notebook:\n",
    "* [Section 1: Preparing Corpus](#s1)\n",
    "* [Section 2: Preparing Training Data](#s2)\n",
    "* [Section 3: Training with Pytorch and Wandb](#s3)\n",
    "* [Section 4: Testing](#s4)\n",
    "* [Section 5: Inference](#s5)\n",
    "* [Section 6: Incorporate Synthetic Vector Into GloVe](#s6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0711e6-c2fd-49eb-90a7-3b4377792bc0",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s1\">Section 1: Preparing Corpus</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c538dce-b95d-44d2-a9b8-aef676355cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 862M/862M [03:39<00:00, 3.93MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
    "\n",
    "dest_file = './glove_6B.zip'\n",
    "# Streaming, so we can iterate over the response.\n",
    "response = requests.get(url, stream=True)\n",
    "total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "block_size = 1024 #1 Kibibyte\n",
    "progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "with open(dest_file, 'wb') as file:\n",
    "    for data in response.iter_content(block_size):\n",
    "        progress_bar.update(len(data))\n",
    "        file.write(data)\n",
    "progress_bar.close()\n",
    "if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "    print(\"ERROR, something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27c0d8e-9546-4627-a431-5afa62c85dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract to folder\n",
    "import zipfile\n",
    "with zipfile.ZipFile(dest_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dest_file.replace('.zip','') )  #use the filename as destination dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "565d5d05-2ed9-49e5-8019-f057df507631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the glove files\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def generate(file):\n",
    "    words = []\n",
    "    vectors = {}\n",
    "    with open(file, 'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            _temp = line.rstrip().split(' ')\n",
    "            words.append(_temp[0])\n",
    "            vectors[_temp[0]] = [float(x) for x in _temp[1:]]\n",
    "\n",
    "    vocab_size = len(words)\n",
    "    vocab = {w: idx for idx, w in enumerate(words)}\n",
    "    ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "\n",
    "    vector_dim = len(vectors[ivocab[0]])\n",
    "    W = np.zeros((vocab_size, vector_dim))\n",
    "    for word, v in vectors.items():\n",
    "        if word == '<unk>':\n",
    "            continue\n",
    "        W[vocab[word], :] = v\n",
    "\n",
    "    # normalize each word vector to unit variance\n",
    "    W_norm = np.zeros(W.shape)\n",
    "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "    W_norm = (W.T / d).T\n",
    "    return (W_norm, vocab, ivocab)\n",
    "\n",
    "glove_file = f\"{dest_file.replace('.zip','')}/glove.6B.100d.txt\"\n",
    "(W_norm, vocab, ivocab) = generate(glove_file)\n",
    "\n",
    "#save the files as npy for easier loading \n",
    "np.save('./glove6B100d.npy',W_norm)\n",
    "with open('./glove_vocab.json','w') as f:\n",
    "    json.dump(vocab,f)\n",
    "with open('./glove_ivocab.json','w') as f:\n",
    "    json.dump(ivocab,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f2aed-194f-475c-8855-07811db53188",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s2\">Section 2: Preparing Training Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bba83f4-0a73-471b-90fa-92fd1be2aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section covers the preparation pipeline. It is assumed that the wikitext is already downloaded and extracted\n",
    "# the wikipedia dump is here https://dumps.wikimedia.org/enwiki/20210920/enwiki-20210920-pages-articles-multistream.xml.bz2 \n",
    "# the source code to extract it is here https://github.com/attardi/wikiextractor \n",
    "\n",
    "\n",
    "#eventually, this part of the code just need the path to all the text files, \n",
    "#so it's up to you to implement it how you like\n",
    "import os, re\n",
    "wiki_dataset_dir = \"D:/DATASETS_UNZIP/Datasets/text\"\n",
    "#Extracting from wiki \n",
    "filepath_list = []\n",
    "wiki=True\n",
    "if wiki == True:\n",
    "    for folder in os.listdir(wiki_dataset_dir):\n",
    "        for file in os.listdir(wiki_dataset_dir+'/'+folder):\n",
    "            filepath_list.append(wiki_dataset_dir+'/'+folder+'/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d085e2a-02c4-444a-905c-88cc0e82b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279e5b3-9b04-4f52-bfa1-30a2a159f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this if you dont have nltk stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5a8b2b-60aa-472c-9822-69976b34a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b606f234-b224-47e1-a297-a9e233d55d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the target word and neighboring word from its sentences\n",
    "\n",
    "def extract(sentence,target_word,context_length,pad,debug=True):\n",
    "    target_word = target_word.lower()\n",
    "    sentence = sentence.lower()\n",
    "    if target_word not in sentence: #reduce processing\n",
    "        return\n",
    "    \n",
    "    s = sentence.lower().strip()\n",
    "    s = re.sub('[\\n\\r\\ ]+',' ',s)\n",
    "    s = re.sub('[^a-z ]+','',s)\n",
    "    \n",
    "    t = target_word\n",
    "    raw_tokens = s.split(' ')\n",
    "    tokens = [i for i in raw_tokens if i != '' and i not in stopword_list]\n",
    "    word_list = []\n",
    "    if t in tokens:\n",
    "        __index = tokens.index(t)        # this only get one utterance, what about other utterances? \n",
    "        if __index < context_length:      #pad front\n",
    "            word_list += [pad for _ in range(context_length-__index)]\n",
    "            word_list += tokens[:__index]      #pad back\n",
    "        else:\n",
    "            word_list += tokens[__index-context_length:__index]\n",
    "        if __index + context_length >= len(tokens):\n",
    "            word_list += tokens[__index+1:]\n",
    "            word_list += [pad for _ in range(context_length + __index + 1 - len(tokens))]\n",
    "        else:\n",
    "            word_list += tokens[__index+1:__index+context_length+1]\n",
    "        return word_list\n",
    "\n",
    "\n",
    "    else:   #target not found:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d93340ad-0721-4b42-a680-687747c1c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda25d1-6547-4957-885b-70c189587c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawling multiple words at once, writing the file into target directory  \n",
    "\n",
    "target_words = ['tired','pointless']\n",
    "                \n",
    "context_length = 10\n",
    "\n",
    "for target_word in target_words:\n",
    "    line_written = 0\n",
    "    corpus = f\"./processed_data/{target_word}_corpus_c{context_length}.txt\"\n",
    "    with open(corpus,'a') as o10:\n",
    "        for file in tqdm(filepath_list):\n",
    "            with open(file,'r',encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    result = extract(line,target_word,context_length,\"<pad>\",True)\n",
    "                    if result == None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        o10.write(f'{target_word}:')\n",
    "                        o10.write(''.join([i+' ' for i in result] ))\n",
    "                        o10.write('\\n')\n",
    "                        line_written += 1\n",
    "                        if line_written % 10000 == 0:\n",
    "                            print(f\"Written {line_written} into file {target_word}\")\n",
    "        print(\"\\n\\n DONE WITH CORPUS {corpus}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58115d9a-b724-4fe7-9c59-22ffb0ef1184",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s3\">Section 3: Training with Pytorch and Wandb</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12431ff-0068-4d47-8a6a-ccd023ac895a",
   "metadata": {},
   "source": [
    "this section is monitored by wandb. Please change the config accordingly if you want to rerun your experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "787a3173-be67-4b96-99e3-d20ab5955052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: sosig_catto (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.12.2 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cool-hill-39</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sosig_catto/Synthetic%20Net\" target=\"_blank\">https://wandb.ai/sosig_catto/Synthetic%20Net</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sosig_catto/Synthetic%20Net/runs/1887l11b\" target=\"_blank\">https://wandb.ai/sosig_catto/Synthetic%20Net/runs/1887l11b</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\hoang\\Desktop\\Light\\Torch_playground\\FYPv2\\demo\\wandb\\run-20210927_174629-1887l11b</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "wandb.init(project='Synthetic Net')\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36c9c146-6b8b-45c3-8280-2cbb31f9ece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (act): ReLU()\n",
      "  (out): Tanh()\n",
      "  (hidden1): Linear(in_features=2000, out_features=2048, bias=True)\n",
      "  (hidden2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (hidden3): Linear(in_features=512, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,context_length,embed_size=100):\n",
    "        super().__init__()\n",
    "        self.n = context_length*2\n",
    "        self.embed_size = embed_size\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Tanh() \n",
    "        self.hidden1 = nn.Linear(self.n*self.embed_size,2048)\n",
    "        self.hidden2 = nn.Linear(2048,512)\n",
    "        self.hidden3 = nn.Linear(512,self.embed_size)\n",
    " \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.act(self.hidden1(x))\n",
    "        x = self.act(self.hidden2(x))\n",
    "        x = self.out(self.hidden3(x))\n",
    "        return x\n",
    "    \n",
    "config.context_length = 10\n",
    "model = DenseNet(context_length = config.context_length)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "614d433d-ea17-46b8-89b6-87092105bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#util file contains the loading functions\n",
    "from util import *\n",
    "\n",
    "#Loading the data\n",
    "W_norm,vocab,ivocab = load_glove(        \n",
    "        weight_file = './glove6B100d.npy',\n",
    "        vocab_file = './glove_vocab.json',\n",
    "        ivocab_file='./glove_ivocab.json'\n",
    ")\n",
    "    \n",
    "config.batch_size = 64\n",
    "\n",
    "#configure the files used for training. Can load multiple files\n",
    "#usually load with negative samples to avoid overfitting\n",
    "files_for_training =['tired'] \n",
    "\n",
    "training_files = [f'./processed_data/{x}_corpus_c10.txt' for x in files_for_training]\n",
    "\n",
    "training_data = load_training_batch(training_files,config.batch_size)\n",
    "\n",
    "#for logging purpose, provide data lineage\n",
    "config.data = \"wiki_only\"\n",
    "\n",
    "train_tensor = get_embedding(training_data,W_norm,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9767b1e4-9a34-44fd-a8c6-03dd95382948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "### checking vocab\n",
    "print(len(vocab))\n",
    "print(len(ivocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7124e79a-c7ea-48ad-b90b-30aa3cc237d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "config.lr = 0.0005\n",
    "config.momentum = 0.005\n",
    "optimizer = optim.SGD(model.parameters(),lr=config.lr,momentum=config.momentum,weight_decay=0.01)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def cosim(v1,v2):\n",
    "    return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, debug_set.shape[0], eta_min=config.lr)\n",
    "#learning rate adjustment -- try 0.001\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        features,labels = batch\n",
    "        batch_size = features.shape[0]\n",
    "        predictions = model(torch.Tensor(features)).squeeze(1)\n",
    "        loss = criterion(predictions,torch.Tensor(labels))      \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        cosim_score = np.mean([cosim(labels[i],predictions[i].detach().numpy()) for i in range(batch_size) ])\n",
    "        \n",
    "    return epoch_loss,cosim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "214fcfe4-081e-4a90-9422-51b97ea7af0a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                                 | 1/40 [00:07<04:48,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:01\t|\tTrain Loss: 18.281\t|\tCosim score: 0.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 2/40 [00:14<04:43,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:02\t|\tTrain Loss: 17.854\t|\tCosim score: 0.194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▏                                                                            | 3/40 [00:22<04:37,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:03\t|\tTrain Loss: 17.436\t|\tCosim score: 0.249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 4/40 [00:30<04:35,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:04\t|\tTrain Loss: 17.026\t|\tCosim score: 0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▍                                                                        | 5/40 [00:38<04:32,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:05\t|\tTrain Loss: 16.624\t|\tCosim score: 0.352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▍                                                                      | 6/40 [00:46<04:28,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:06\t|\tTrain Loss: 16.229\t|\tCosim score: 0.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████▌                                                                    | 7/40 [00:54<04:22,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:07\t|\tTrain Loss: 15.840\t|\tCosim score: 0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 8/40 [01:02<04:17,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:08\t|\tTrain Loss: 15.458\t|\tCosim score: 0.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████▋                                                                | 9/40 [01:10<04:10,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:09\t|\tTrain Loss: 15.083\t|\tCosim score: 0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████▌                                                             | 10/40 [01:19<04:04,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10\t|\tTrain Loss: 14.713\t|\tCosim score: 0.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▌                                                           | 11/40 [01:27<03:55,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11\t|\tTrain Loss: 14.349\t|\tCosim score: 0.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▌                                                         | 12/40 [01:35<03:48,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:12\t|\tTrain Loss: 13.992\t|\tCosim score: 0.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▋                                                       | 13/40 [01:43<03:38,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:13\t|\tTrain Loss: 13.640\t|\tCosim score: 0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████████████████████▋                                                     | 14/40 [01:51<03:30,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:14\t|\tTrain Loss: 13.294\t|\tCosim score: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████▊                                                   | 15/40 [01:59<03:22,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:15\t|\tTrain Loss: 12.955\t|\tCosim score: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▊                                                 | 16/40 [02:08<03:15,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:16\t|\tTrain Loss: 12.622\t|\tCosim score: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████▊                                               | 17/40 [02:16<03:07,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:17\t|\tTrain Loss: 12.295\t|\tCosim score: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████▉                                             | 18/40 [02:24<02:59,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:18\t|\tTrain Loss: 11.975\t|\tCosim score: 0.749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████▉                                           | 19/40 [02:32<02:51,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:19\t|\tTrain Loss: 11.661\t|\tCosim score: 0.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████                                         | 20/40 [02:40<02:42,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20\t|\tTrain Loss: 11.354\t|\tCosim score: 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████                                       | 21/40 [02:48<02:34,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:21\t|\tTrain Loss: 11.053\t|\tCosim score: 0.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████                                     | 22/40 [02:56<02:27,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:22\t|\tTrain Loss: 10.760\t|\tCosim score: 0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████████████▏                                  | 23/40 [03:05<02:18,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:23\t|\tTrain Loss: 10.473\t|\tCosim score: 0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▏                                | 24/40 [03:13<02:12,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:24\t|\tTrain Loss: 10.192\t|\tCosim score: 0.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████▎                              | 25/40 [03:22<02:04,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:25\t|\tTrain Loss: 9.918\t|\tCosim score: 0.832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████████████████████████████████▎                            | 26/40 [03:30<01:56,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:26\t|\tTrain Loss: 9.651\t|\tCosim score: 0.841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████████████████████████████████████▎                          | 27/40 [03:40<01:55,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:27\t|\tTrain Loss: 9.391\t|\tCosim score: 0.849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████▍                        | 28/40 [03:49<01:46,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:28\t|\tTrain Loss: 9.138\t|\tCosim score: 0.856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████▍                      | 29/40 [03:58<01:37,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:29\t|\tTrain Loss: 8.893\t|\tCosim score: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████▌                    | 30/40 [04:06<01:27,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:30\t|\tTrain Loss: 8.655\t|\tCosim score: 0.870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████▌                  | 31/40 [04:15<01:18,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31\t|\tTrain Loss: 8.424\t|\tCosim score: 0.876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 32/40 [04:24<01:09,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:32\t|\tTrain Loss: 8.202\t|\tCosim score: 0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████████▋              | 33/40 [04:32<01:00,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:33\t|\tTrain Loss: 7.987\t|\tCosim score: 0.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 34/40 [04:40<00:50,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:34\t|\tTrain Loss: 7.779\t|\tCosim score: 0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 35/40 [04:49<00:42,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:35\t|\tTrain Loss: 7.579\t|\tCosim score: 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 36/40 [04:57<00:33,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:36\t|\tTrain Loss: 7.386\t|\tCosim score: 0.902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████▊      | 37/40 [05:05<00:24,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:37\t|\tTrain Loss: 7.199\t|\tCosim score: 0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 38/40 [05:13<00:16,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:38\t|\tTrain Loss: 7.019\t|\tCosim score: 0.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████████████████████████████████████▉  | 39/40 [05:21<00:08,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:39\t|\tTrain Loss: 6.844\t|\tCosim score: 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [05:29<00:00,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:40\t|\tTrain Loss: 6.676\t|\tCosim score: 0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config.epochs = 40   #usually 40 is the best\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(config.epochs)):   \n",
    "    train_loss,cosim_score= train(model,iter(train_tensor), optimizer, criterion)\n",
    "\n",
    "    #epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    #if valid_loss < best_valid_loss:\n",
    "     #   best_valid_loss = valid_loss\n",
    "      #  torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    #print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    wandb.log({\"loss\":train_loss,\"cosim_score\":cosim_score})\n",
    "    print(f'Epoch:{epoch+1:02}\\t|\\tTrain Loss: {train_loss:.3f}\\t|\\tCosim score: {cosim_score:.3f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31393ac2-be1f-4622-bed1-a3a43ce8f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir output\n",
    "torch.save(model.state_dict(),f'output/{date.today().strftime(\"%Y-%m\")}_{config.data}_{wandb.run.name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261e270a-ce19-4a11-84f3-75931f23852a",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s4\">Section 4: Testing</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46ab9c27-8f33-4f32-947f-052f6e74ef68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "Test 1 -- sample sentence: \n",
      "\n",
      "tired:short run run sweet road runner real scored music ten feathered clippety clobbered used set generic musical cues follow action \n",
      "\n",
      "\n",
      "\n",
      "(400000,)\n",
      "\n",
      "                               Word       Unnormalized Cosine distance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "0                    multilateralism\t\t0.190550\n",
      "1                          resurging\t\t0.161833\n",
      "2                        replicators\t\t0.156357\n",
      "3                       undiminished\t\t0.147284\n",
      "4                            sinning\t\t0.145458\n",
      "5                            finning\t\t0.143951\n",
      "6                             glatch\t\t0.143233\n",
      "7                      neoliberalism\t\t0.142868\n",
      "8                      americanizing\t\t0.142485\n",
      "9                          labarbera\t\t0.142431\n",
      "\n",
      "\n",
      "\t\tCosim score: [0.09054023]\n"
     ]
    }
   ],
   "source": [
    "## section 4: Testing using analogy tests\n",
    "model_to_test =  DenseNet(context_length = 10)\n",
    "model_to_test.load_state_dict(torch.load('./output/2021-09_wiki_only_cool-hill-39.pt'))\n",
    "\n",
    "##Testing has 3 unit tests\n",
    "#Test 1 sentence\n",
    "#Test 1 batch\n",
    "#custom sentence\n",
    "\n",
    "\n",
    "#Test 1 -- random sentence in training\n",
    "random_sent = random.choice(training_data)[random.randint(0,config.batch_size-1)]\n",
    "y,x = random_sent.split(':')\n",
    "x = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "sample_tensor = torch.Tensor([[get_glove_vec(word,W_norm,vocab) for word in x.split(' ')]])\n",
    "sample_output = model_to_test(sample_tensor)\n",
    "target_label = np.array(get_glove_vec(y,W_norm,vocab))\n",
    "\n",
    "output1 = sample_output.squeeze(1)\n",
    "vec_output1 = output.detach().numpy()\n",
    "print(vec_output1.shape)\n",
    "\n",
    "def __distance(W, vocab, ivocab, vec_output):\n",
    "\n",
    "\n",
    "    dist = np.dot(W, vec_output.T).squeeze(1)\n",
    "    print(dist.shape)\n",
    "    a = np.argsort(-dist)[:10]\n",
    "\n",
    "    print(\"\\n                               Word       Unnormalized Cosine distance\\n\")\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    for i,x in enumerate(a):\n",
    "        print(\"%d%35s\\t\\t%f\" % (i,ivocab[str(x)], dist[x]))\n",
    "print(f\"Test 1 -- sample sentence: \\n\\n{random_sent}\\n\\n\")\n",
    "\n",
    "__distance(W_norm,vocab,ivocab,vec_output1)\n",
    "\n",
    "\n",
    "print(f\"\\n\\n\\t\\tCosim score: {cosim(vec_output1,target_label)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3caef383-4354-4190-bcc6-8ed52d9c76af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2 -- sample batch: \n",
      "\n",
      "\n",
      "(400000,)\n",
      "\n",
      "                               Word       Unnormalized Cosine distance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "0                              tired\t\t0.596813\n",
      "1                             scared\t\t0.462481\n",
      "2                              weary\t\t0.460485\n",
      "3                                 'm\t\t0.446562\n",
      "4                         frustrated\t\t0.442676\n",
      "5                           fatigued\t\t0.438139\n",
      "6                          exhausted\t\t0.436740\n",
      "7                              bored\t\t0.434672\n",
      "8                             afraid\t\t0.433339\n",
      "9                               feel\t\t0.431909\n",
      "\n",
      "\n",
      "\t\tCosim score: [0.14825795]\n"
     ]
    }
   ],
   "source": [
    "#test 2: test by batch\n",
    "\n",
    "random_batch = random.choice(training_data)\n",
    "sample_batch_tensor = []\n",
    "target_batch_tensor = []\n",
    "for sentence in random_batch:\n",
    "    y,x = sentence.split(':')\n",
    "    x = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "    sample_tensor = [get_glove_vec(word,W_norm,vocab) for word in x.split(' ')]\n",
    "    target_batch_tensor.append(get_glove_vec(y,W_norm,vocab))\n",
    "    sample_batch_tensor.append(sample_tensor)\n",
    "    \n",
    "sample_batch_tensor = torch.Tensor(np.array(sample_batch_tensor))\n",
    "target_batch_tensor = np.array(target_batch_tensor)\n",
    "\n",
    "sample_output = model(sample_batch_tensor)\n",
    "\n",
    "output2 = torch.mean(sample_output,0)   #sum across embeddings\n",
    "vec_output2 = output2.detach().numpy().reshape((1,100))\n",
    "\n",
    "print(f\"Test 2 -- sample batch: \\n\\n\")\n",
    "\n",
    "__distance(W_norm,vocab,ivocab,vec_output2)\n",
    "\n",
    "\n",
    "print(f\"\\n\\n\\t\\tCosim score: {cosim(vec_output2,target_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa79a25f-c409-4d12-ac6e-2bdebdb494a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3: Custom Test\n",
      "\n",
      "pacific disaster response fund support armenian government fight spread covid year bank committed million loan electric networks armenia ensure electricity\n",
      "\n",
      "\n",
      "(400000,)\n",
      "\n",
      "                               Word       Unnormalized Cosine distance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "0                    multilateralism\t\t0.190550\n",
      "1                          resurging\t\t0.161833\n",
      "2                        replicators\t\t0.156357\n",
      "3                       undiminished\t\t0.147284\n",
      "4                            sinning\t\t0.145458\n",
      "5                            finning\t\t0.143951\n",
      "6                             glatch\t\t0.143233\n",
      "7                      neoliberalism\t\t0.142868\n",
      "8                      americanizing\t\t0.142485\n",
      "9                          labarbera\t\t0.142431\n",
      "\n",
      "\n",
      "\t\tCosim score: [0.12973318]\n"
     ]
    }
   ],
   "source": [
    "###Test 3: Custom\n",
    "\n",
    "\n",
    "random_sent = 'pacific disaster response fund support armenian government fight spread covid year bank committed million loan electric networks armenia ensure electricity '\n",
    "target_word = 'pneumonia'\n",
    "target_label = np.array(get_glove_vec(target_word,W_norm,vocab))\n",
    "random_sent = re.sub('[\\n\\r\\ ]+',' ',random_sent).strip()\n",
    "\n",
    "sample_tensor = torch.Tensor([[get_glove_vec(word,W_norm,vocab) for word in random_sent.split(' ')]])\n",
    "sample_output = model(sample_tensor)\n",
    "output3 = sample_output.squeeze(1)\n",
    "vec_output3 = output3.detach().numpy()\n",
    "print(f\"Test 3: Custom Test\\n\\n{random_sent}\\n\\n\")\n",
    "__distance(W_norm,vocab,ivocab,vec_output3)\n",
    "print(f\"\\n\\n\\t\\tCosim score: {cosim(vec_output3,target_label)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9cba6-5f8d-4b85-91b1-02143a6a347e",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s5\">Section 5: Inference</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac467452-f105-4d95-b330-6656ad92ade7",
   "metadata": {},
   "source": [
    "**Note**: the driver function is <a class=\"anchor\" id=\"driver\">get_google_search_page</a>. The driver is currently defined as Firefox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d3cfce-928f-4af4-a200-304993637f9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the google page...\n",
      "Get the first page results....\n",
      "Duplicate and filter to 100 pages...\n",
      "Extracting the h1 and p elements from these pages...\n",
      "ERROR at url https://www.straitstimes.com/world/heart-inflammation-rates-higher-after-moderna-covid-19-shot-than-pfizer-vaccine-canada-data\n",
      "Writing to file\n",
      "Finishing with url: https://www.channelnewsasia.com/asia/japan-takeda-moderna-covid-19-vaccine-contaminant-human-error-recall-2214936\n",
      "Finishing with url: https://www.pharmaceutical-technology.com/comment/moderna-vaccine-recalled-novovax-replacement/\n",
      "Finishing with url: https://www.todayonline.com/singapore/singapore-sends-100000-doses-moderna-covid-19-vaccine-brunei\n",
      "Finishing with url: https://www.investing.com/news/stock-market-news/moderna-biontech-pfizer-fall-on-merck-covid19-pill-news-2632204\n",
      "Finishing with url: https://www.businessinsider.com/moderna-vaccines-might-not-need-boosters-like-pfizer-2021-9\n",
      "Finishing with url: https://www.latimes.com/science/story/2021-09-17/study-finds-big-gap-between-pfizer-moderna-vaccines-at-preventing-covid-hospitalizations\n",
      "Finishing with url: https://www.bbc.com/news/world-asia-58338281\n",
      "Finishing with url: https://www.modernatx.com/modernas-work-potential-vaccine-against-covid-19\n",
      "Finishing with url: https://www.cdc.gov/coronavirus/2019-ncov/vaccines/different-vaccines/Moderna.html\n",
      "Finishing with url: https://www.fda.gov/advisory-committees/advisory-committee-calendar/vaccines-and-related-biological-products-advisory-committee-december-17-2020-meeting-announcement\n",
      "Finishing with url: https://www.bloomberg.com/quote/MRNA:US\n",
      "Finishing with url: https://www.who.int/news-room/feature-stories/detail/the-moderna-covid-19-mrna-1273-vaccine-what-you-need-to-know\n",
      "Finishing with url: https://www.nytimes.com/2021/09/22/health/covid-moderna-pfizer-vaccines.html\n",
      "Document ready\n",
      "Getting the google page...\n",
      "Get the first page results....\n",
      "Duplicate and filter to 100 pages...\n",
      "Extracting the h1 and p elements from these pages...\n",
      "ERROR at url https://www.straitstimes.com/world/united-states/astrazeneca-covid-19-vaccine-shows-74-efficacy-in-large-us-trial\n",
      "Writing to file\n",
      "Finishing with url: https://www.astrazeneca.com/country-sites/singapore.html\n",
      "Finishing with url: https://www.bbc.com/news/uk-england-tyne-58330796\n",
      "Finishing with url: https://www.newscientist.com/article/2237475-covid-19-news-study-finds-benefits-of-teen-vaccination-outweigh-risks/\n",
      "Finishing with url: https://www.reuters.com/business/healthcare-pharmaceuticals/two-doses-pfizer-astrazeneca-shots-effective-against-delta-variant-study-finds-2021-07-21/\n",
      "Finishing with url: https://www.healthline.com/health/astrazeneca-vs-pfizer-vaccine\n",
      "Finishing with url: https://www.nature.com/articles/d41586-021-02291-2\n",
      "Finishing with url: https://www.wsj.com/articles/astrazeneca-breast-cancer-drug-found-to-reduce-risk-of-dying-11632146777\n",
      "Finishing with url: https://www.dw.com/en/coronavirus-digest-astrazeneca-vaccine-still-safe-says-who/a-57023010\n",
      "Finishing with url: https://www.astrazeneca-us.com/media.html\n",
      "Finishing with url: https://www.who.int/news/item/07-04-2021-interim-statement-of-the-covid-19-subcommittee-of-the-who-global-advisory-committee-on-vaccine-safety\n",
      "Finishing with url: https://www.health.gov.au/initiatives-and-programs/covid-19-vaccines/approved-vaccines/astrazeneca\n",
      "Finishing with url: https://www.linkedin.com/company/astrazeneca\n",
      "Document ready\n",
      "Getting the google page...\n",
      "Get the first page results....\n",
      "Duplicate and filter to 100 pages...\n",
      "Extracting the h1 and p elements from these pages...\n",
      "Writing to file\n",
      "Finishing with url: https://www.washington.edu/\n",
      "Finishing with url: https://www.washingtonpost.com/coronavirus/\n",
      "Finishing with url: https://www.washington.org/\n",
      "Finishing with url: https://www.experiencewa.com/\n",
      "Document ready\n",
      "Getting the google page...\n",
      "Get the first page results....\n",
      "Duplicate and filter to 100 pages...\n",
      "Extracting the h1 and p elements from these pages...\n",
      "Writing to file\n",
      "Finishing with url: https://www.raffles.com/\n",
      "Finishing with url: https://www.rafflessingapore.com/\n",
      "Finishing with url: https://www.rafflesmedicalgroup.com/services/hospital/\n",
      "Finishing with url: https://www.raffleshealth.com/\n",
      "Finishing with url: https://www.rgs.edu.sg/\n",
      "Finishing with url: https://www.britannica.com/biography/Stamford-Raffles\n",
      "Finishing with url: https://www.booking.com/hotel/sg/raffles.html\n",
      "Document ready\n",
      "Getting the google page...\n",
      "Get the first page results....\n",
      "Duplicate and filter to 100 pages...\n",
      "Extracting the h1 and p elements from these pages...\n",
      "Writing to file\n",
      "Finishing with url: https://www.sysnmh.org.sg/en/whats-on/events/public-lecture---tales-of-tekong-i--telling-the-story-of-tekong-through-historical-maps\n",
      "Finishing with url: https://www.deltares.nl/en/projects/singapore-to-adopt-dutch-polder-concept-as-new-land-reclamation-method-at-pulau-tekong\n",
      "Finishing with url: https://www.todayonline.com/topics/pulau-tekong\n",
      "Document ready\n",
      "Getting the google page...\n",
      "Get the first page results....\n",
      "Duplicate and filter to 100 pages...\n",
      "Extracting the h1 and p elements from these pages...\n",
      "Writing to file\n",
      "Finishing with url: https://www.whitehouse.gov/about-the-white-house/presidents/barack-obama/\n",
      "Finishing with url: https://www.barackobama.com/\n",
      "Finishing with url: https://www.cnn.com/videos/politics/2021/09/29/barack-obama-warning-dons-take-sot-dlt-vpx.cnn\n",
      "Finishing with url: https://www.wsj.com/articles/obama-wins-another-victory-against-community-organizers-11632944791\n",
      "Finishing with url: https://www.ndtv.com/world-news/outdated-white-house-situation-room-is-getting-a-needed-overhaul-2560077\n",
      "Finishing with url: https://www.msnbc.com/the-beat-with-ari/watch/obama-vet-chai-komanduri-blasts-joe-manchin-and-kyrsten-sinema-for-revealing-policy-conditions-for-spending-package-122235973677\n",
      "Finishing with url: https://www.theguardian.com/us-news/2021/sep/28/barack-obama-tax-rich-personal-wealth-biden-spending-plan\n",
      "Finishing with url: https://www.nytimes.com/topic/person/barack-obama\n",
      "Finishing with url: https://www.dailymail.co.uk/femail/article-10026163/Meghan-Markle-channels-Michelle-Obamas-inauguration-outfit-New-York.html\n",
      "Finishing with url: https://www.reuters.com/world/us-ex-presidents-bush-clinton-obama-band-together-aid-afghan-refugees-2021-09-14/\n",
      "Finishing with url: https://www.obama.org/\n",
      "Finishing with url: https://www.facebook.com/barackobama/\n",
      "Finishing with url: https://www.britannica.com/biography/Barack-Obama\n",
      "Document ready\n",
      "Getting the google page...\n",
      "Get the first page results....\n",
      "Duplicate and filter to 100 pages...\n",
      "Extracting the h1 and p elements from these pages...\n",
      "Writing to file\n",
      "Finishing with url: https://www.bloomberg.com/news/articles/2021-09-15/new-york-city-rents-landlords-jack-up-prices-70-in-lease-renewals-post-covid\n",
      "Finishing with url: https://www.bbc.com/news/world-us-canada-58683002\n",
      "Finishing with url: https://www.harpersbazaar.com/celebrity/latest/a37704411/prince-harry-meghan-markle-nyc-deblasio-visit/\n",
      "Finishing with url: https://www.nbcnews.com/news/us-news/federal-judge-deals-blow-covid-19-vaccine-mandate-nyc-teachers-n1280097\n",
      "Finishing with url: https://www.dezeen.com/2021/09/24/summit-one-vanderbilt-observation-deck-new-york-city/\n",
      "Finishing with url: https://www.timesofisrael.com/for-bennett-sukkot-in-nyc-is-a-time-to-stroll-down-memory-lane-and-5th-ave/\n",
      "Finishing with url: https://www.nyc.gov.sg/en/initiatives/programmes/\n",
      "Finishing with url: https://www.nyc.gov/\n",
      "Finishing with url: https://www.nycgo.com/\n",
      "Finishing with url: https://www.facebook.com/nycsg\n",
      "Finishing with url: https://www.nbcnewyork.com/news/weird/man-walking-on-nyc-street-attacked-by-roosters-that-have-terrorized-neighbors-for-years/3301433/\n",
      "Finishing with url: https://www.sgdi.gov.sg/ministries/mccy/departments/nyc\n",
      "Document ready\n"
     ]
    }
   ],
   "source": [
    "## section 5: generating for an unknown word \n",
    "\n",
    "#we will crawl some data for an unknown words using selenium, and export it to a text file\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pprint\n",
    "import time\n",
    "import argparse\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_google_search_page(input_text):\n",
    "    fox = webdriver.Firefox()\n",
    "    fox.get(f\"https://www.google.com/search?q={input_text}\")\n",
    "    search_bar = fox.find_element_by_tag_name(\"input\")\n",
    "    time.sleep(2)\n",
    "    cur_url = fox.current_url\n",
    "    fox.close()\n",
    "    fox.quit()\n",
    "    return cur_url\n",
    "\n",
    "def get_top_results(URL):\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    all_urls = []\n",
    "    for elems in soup.findAll('h3'):\n",
    "        for link in soup.findAll('a'):\n",
    "            url = str(link.get('href'))\n",
    "            if url.startswith('/url?'):\n",
    "                all_urls.append(url[7:])\n",
    "    return all_urls\n",
    "\n",
    "def filter_duplicate(url_list,max_url=5):\n",
    "    domain_list = set()\n",
    "    result = dict()\n",
    "    blacklist = [\"youtube.com\"]\n",
    "    for url in url_list:\n",
    "        domain = re.findall(r'^https://www.(.*?)/',url) \n",
    "        url = re.findall(r'^https://(.*?)&sa',url) # &sa is there for the google rubbish\n",
    "        if domain != [] and domain[0] not in domain_list and domain[0] not in blacklist:   #it will not check[0] until domain != [] satisfy\n",
    "            domain_list.add('https://www.'+domain[0])\n",
    "            result['www.'+domain[0]] = \"https://\"+url[0]\n",
    "            if len(domain_list) == max_url:\n",
    "                break\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_elems_from_urls(urls):\n",
    "    stored_text = dict()\n",
    "    for url_ in urls.values():\n",
    "        #print(f'========{url_}===========')\n",
    "        try: \n",
    "            childpage = requests.get(url_)\n",
    "            childsoup = BeautifulSoup(childpage.content, 'html.parser')\n",
    "            stored_text[url_] = {'h1':set(),'p':set()}\n",
    "            for tagment in ['h1','p']:\n",
    "                    elements = childsoup.findAll(tagment)\n",
    "                    for element in elements:\n",
    "                        stored_text[url_][tagment].add(element.text)\n",
    "        except:\n",
    "            print(f\"ERROR at url {url_}\")\n",
    "            pass\n",
    "        #print(\"========================================================\")\n",
    "    return stored_text\n",
    "\n",
    "\n",
    "def clean_sentence(sent):\n",
    "    sent = re.sub(f'[\\ \\r\\n]+',' ',sent)\n",
    "    return sent.lower()\n",
    "\n",
    "\n",
    "def write_text_dict_to_file(stored_text,outputfile='test.txt'):\n",
    "    with open(outputfile,'w',encoding='utf-8') as f:\n",
    "        for url,texts in stored_text.items():\n",
    "            f.write(f'=========={url}===========\\n')\n",
    "            clean_h1 = []\n",
    "            f.write(f'<<h1>>\\n')\n",
    "            for h1 in texts['h1']:\n",
    "                clean_h1 += sent_tokenize(h1)\n",
    "            for sentence in clean_h1:\n",
    "                f.write(clean_sentence(sentence))\n",
    "                f.write('\\n')\n",
    "            f.write(f'<<p>>\\n')\n",
    "            clean_p = []\n",
    "            for p in texts['p']:\n",
    "                clean_p += sent_tokenize(p)\n",
    "            for sentence in clean_p:\n",
    "                f.write(clean_sentence(sentence))\n",
    "                f.write('\\n')\n",
    "            f.write('\\n\\n')\n",
    "            print(f\"Finishing with url: {url}\")\n",
    "    print(\"Document ready\")\n",
    "\n",
    "\n",
    "def main(input_text,output,num_pages=10):\n",
    "    print(\"Getting the google page...\")\n",
    "    gg_url = get_google_search_page(input_text)\n",
    "    print(\"Get the first page results....\")\n",
    "    top_search = get_top_results(gg_url) \n",
    "    print(f\"Duplicate and filter to {num_pages} pages...\")\n",
    "    unique_top_search = filter_duplicate(top_search,num_pages)\n",
    "    print(\"Extracting the h1 and p elements from these pages...\")\n",
    "    stored_text = extract_text_elems_from_urls(unique_top_search)\n",
    "    print(\"Writing to file\")\n",
    "    write_text_dict_to_file(stored_text,output)\n",
    "\n",
    "    \n",
    "for i in ['moderna','astrazenecca','washington','raffles','tekong','obama','NYC']:\n",
    "    main(i,f'./inference/{i}.txt',num_pages = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cc016c7-a15b-4624-8604-327e8e0c36fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 362/362 [00:00<00:00, 122357.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time', 'visiting', 'city', 'together', 'meghan', 'baby', 'shower', 'son', 'archie', 'mountbattenwindsor', 'back', 'harry', 'london', 'engagement', 'highly', 'anticipated', 'visit', 'also', 'duchesss', 'first']\n",
      "['<pad>', '<pad>', '<pad>', 'federal', 'judge', 'deals', 'blow', 'covid', 'vaccine', 'mandate', 'teachers', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'bennett', 'sukkot', 'time', 'stroll', 'memory', 'lane', 'th', 'ave', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'also', 'part', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'fair', 'fares', 'eligible', 'city', 'residents', 'may', 'buy', 'day', 'day', 'pass', 'half', 'price']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'man', 'walking', 'street', 'attacked', 'rooster', 'terrorized', 'neighbors', 'years', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 399/399 [00:00<00:00, 13304.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'barack', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'married', 'michelle', 'robinson', 'lawyer', 'also', 'excelled', 'harvard', 'law', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'elected', 'illinois', 'senate', 'us', 'senate', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'barack', 'elected', 'president', 'became', 'first', 'african', 'american', 'hold', 'office', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'learn', 'barack', 'obamas', 'spouse', 'michelle', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'barack', 'served', 'th', 'president', 'united', 'states', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'incoming', 'president', 'faced', 'many', 'challengesan', 'economic', 'collapse', 'wars', 'iraq', 'afghanistan', 'continuing', 'menace']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'inaugurated', 'estimated', 'crowd', 'million', 'people', 'proposed', 'unprecedented', 'federal', 'spending', 'revive', 'economy', 'also', 'hoped', 'renew', 'americas']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'also', 'pressed', 'fair', 'pay', 'act', 'women', 'financial', 'reform', 'legislation', 'efforts']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'became', 'fourth', 'president', 'receive', 'nobel', 'peace', 'prize', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'last', 'year', 'second', 'term', 'spoke', 'two', 'events', 'clearly', 'moved', 'himthe', 'th', 'anniversary', 'civil', 'rights']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'overseen', 'killing', 'osama', 'bin', 'laden', 'new', 'selfproclaimed', 'islamic', 'state', 'arose']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'sought', 'manage', 'hostile', 'iran', 'treaty', 'hindered', 'development', 'nuclear', 'weapons', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'administration', 'also', 'adopted', 'climate', 'change', 'agreement', 'signed', 'nations', 'reduce', 'greenhouse']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'memoir', 'dreams', 'father', 'describes', 'complexities', 'discovering', 'identity', 'adolescence', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'following', 'graduation', 'worked', 'new', 'york', 'city', 'became', 'community', 'organizer', 'south', 'side', 'chicago']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'office', 'barack', 'michelle', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'shares', 'warning', 'america', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'situation', 'room', 'watched', 'bin', 'laden', 'raid', 'getting', 'overhaul', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'image', 'shows', 'top', 'advisers', 'crammed', 'together', 'small', 'room', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['house', 'photographer', 'pete', 'souza', 'called', 'frame', 'taken', 'may', 'president', 'barack', 'national', 'security', 'team', 'watching', 'live', 'drone', 'video', 'beamed', 'laptop', 'listening']\n",
      "['<pad>', '<pad>', '<pad>', 'said', 'looked', 'around', 'found', 'whoa', 'everybodys', 'including', 'let', 'webb', 'keep', 'seat', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'hardball', 'calvinball', 'vet', 'blasts', 'manchin', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'barack', 'tax', 'rich', 'including', 'fund', 'biden', 'spending', 'plan', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'biden', 'vicepresident', 'remains', 'broadly', 'popular', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'barack', 'says', 'wealthy', 'americans', 'including', 'afford', 'tax', 'increases', 'help', 'fund', 'joe']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'reckoned', 'personal', 'fortune', 'around', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['white', 'house', 'released', 'bestselling', 'memoir', 'part', 'book', 'deal', 'wife', 'michelle', 'signed', 'netflix', 'deal', 'thought', 'worth', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'think', 'afford', 'told', 'abcs', 'good', 'morning', 'america', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['youre', 'talking', 'us', 'stepping', 'spending', 'money', 'providing', 'childcare', 'tax', 'credits', 'said', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'barack', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'former', 'president', 'barack', 'along', 'michelle', 'obama', 'former', 'first', 'lady', 'broke', 'ground', 'presidential', 'center']\n",
      "['markle', 'completed', 'look', 'black', 'red', 'face', 'mask', 'looked', 'like', 'one', 'paired', 'suit', 'eight', 'months', 'ago', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'second', 'thought', 'needed', 'michelle', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'cut', 'stylish', 'figurein', 'headtotoe', 'plum', 'suit', 'sergio', 'hudson', 'watchedbiden', 'vice']\n",
      "['<pad>', '<pad>', 'madame', 'duchess', 'maroon', 'outfit', 'flowy', 'hair', 'giving', 'michelle', 'inauguration', 'someone', 'else', 'agreed', 'someone', 'else', 'added', 'giving', 'michelle', 'obama']\n",
      "['friday', 'point', 'duchess', 'sussexs', 'suit', 'looked', 'lot', 'like', 'one', 'worn', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'meghan', 'markle', 'earning', 'comparisons', 'michelle', 'stepping', 'monochromatic', 'suit', 'looks', 'quite', 'similar', 'former', 'first', 'ladys', 'inauguration']\n",
      "['suits', 'star', 'reimagining', 'royal', 'style', 'template', 'dressing', 'postpublic', 'life', 'set', 'many', 'ways', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'markle', 'appeared', 'channeling', 'berrycolored', 'ensemble', 'twitter', 'users', 'took', 'notice', 'one', 'person', 'joking', 'royal']\n",
      "['twitter', 'users', 'noticed', 'look', 'similar', 'headtotoe', 'plum', 'suit', 'sergio', 'hudson', 'wore', 'president', 'joe', 'bidens', 'inauguration', 'january', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'opening', 'choice', 'explained', 'embodies', 'everything', 'looking', 'interview', 'subject', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'markle', 'certainly', 'admires', 'something', 'made', 'clear', 'past', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'giving', 'michelle', 'inauguration', 'look', 'one', 'person', 'wrote', 'adding', 'messy', 'outfit', 'looks', 'similar']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'one', 'person', 'joked', 'cosplaying', 'another', 'said', 'outfit', 'giving', 'michelle', 'obama', 'inauguration', 'vibes', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'us', 'expresidents', 'bush', 'clinton', 'band', 'together', 'aid', 'afghan', 'refugees', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['us', 'presidents', 'republican', 'george', 'w', 'bush', 'democrats', 'bill', 'clinton', 'barack', 'banded', 'together', 'behind', 'new', 'group', 'aimed', 'supporting', 'refugees', 'afghanistan', 'settling']\n",
      "['sign', 'receive', 'email', 'updates', 'innovative', 'workforce', 'initiative', 'community', 'programming', 'even', 'presidential', 'center', 'special', 'announcements', 'first', 'looks', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'foundations', 'mission', 'inspire', 'empower', 'connect', 'people', 'change', 'world', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'today', 'officially', 'broke', 'ground', 'presidential', 'center', 'south', 'side', 'chicago', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'michelle', 'couldnt', 'excited', 'officially', 'break', 'ground', 'presidential', 'center', 'south', 'side', 'chicago', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'always', 'grateful', 'presidential', 'center', 'way', 'repaying', 'city', 'given', 'us', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'presidential', 'center', 'way', 'showing', 'young', 'people', 'everywhere', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'barack', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'receiving', 'law', 'degree', 'moved', 'chicago', 'became', 'active', 'democratic', 'party', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'period', 'wrote', 'first', 'book', 'saw', 'published', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'lectured', 'constitutional', 'law', 'university', 'chicago', 'worked', 'attorney', 'civil', 'rights', 'issues']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'obamas', 'father', 'barack', 'sr', 'teenage', 'goatherd', 'rural', 'kenya', 'scholarship', 'study', 'united', 'states', 'eventually']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'barack', 'first', 'african', 'american', 'president', 'united', 'states', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'barack', 'graduated', 'punahou', 'school', 'elite', 'academy', 'honolulu', 'attended', 'occidental', 'college', 'transferring']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'age', 'two', 'barack', 'sr', 'left', 'study', 'harvard', 'university', 'shortly', 'thereafter']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'saw', 'father', 'one', 'time', 'brief', 'visit', 'obama', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'lived', 'several', 'years', 'jakarta', 'half', 'sister', 'mother', 'stepfather', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'attended', 'governmentrun', 'school', 'received', 'instruction', 'islam', 'catholic', 'private', 'school', 'took']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'attended', 'occidental', 'college', 'suburban', 'los', 'angeles', 'two', 'years', 'transferred', 'columbia']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'influenced', 'professors', 'pushed', 'take', 'studies', 'seriously', 'experienced', 'great', 'intellectual', 'growth', 'college', 'couple', 'years', 'thereafter', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', 'summer', 'associate', 'chicago', 'law', 'firm', 'sidley', 'austin', 'met', 'chicago', 'native', 'michelle', 'robinson', 'young', 'lawyer', 'firm', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'working', 'writer', 'editor', 'manhattan', 'barack', 'became', 'community', 'organizer', 'chicago', 'lectured', 'constitutional', 'law', 'university', 'chicago', 'worked']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'father', 'barack', 'sr', 'kenyan', 'became', 'economist', 'government', 'kenya', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'barack', 'full', 'barack', 'hussein', 'obama', 'ii', 'born', 'august', 'honolulu', 'hawaii', 'us']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'winning', 'presidency', 'represented', 'illinois', 'us', 'senate', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'graduated', 'punahou', 'school', 'elite', 'college', 'preparatory', 'academy', 'honolulu', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 406/406 [00:00<00:00, 16918.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'seychelles', 'praslin', 'seychelles', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'part', 'accor', 'copyright', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'singapore', 'singapore', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'creative', 'expression', 'longlasting', 'inspiration', 'combine', 'issue', 'magazine', 'inspired', 'five', 'senses', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'poland', 'warsaw', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'philippines', 'makati', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'france', 'le', 'royal', 'monceau', 'paris', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'dubai', 'dubai', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'indonesia', 'jakarta', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'cambodia', 'phnom', 'penh', 'le', 'royal', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'china', 'shenzhen', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'began', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'maldives', 'maldives', 'meradhoo', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'hotel', 'singapore', 'officially', 'reopened', 'line', 'singapore', 'governments', 'safety', 'measures', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'hospital', 'singapore', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['ministry', 'healths', 'emergency', 'care', 'collaboration', 'receive', 'subsidised', 'patients', 'scdf', 'since', 'hospital', 'also', 'accredited', 'certified', 'trauma', 'hospital', 'national', 'trauma', 'committee', 'june']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'copyright', 'medical', 'group', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'hospital', 'hospital', 'built', 'around', 'needs', 'patients', 'providing', 'specialist', 'services', 'combined']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'tertiary', 'care', 'hospital', 'flagship', 'medical', 'group', 'leading', 'private', 'healthcare', 'provider', 'singapore', 'south', 'east', 'asia']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'effective', 'may', 'ministry', 'health', 'moh', 'engaged', 'hospital', 'offer', 'free', 'covid', 'polymerase', 'chain', 'reaction', 'pcr', 'swab', 'tests']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'featured', 'basic', 'tdap', 'doctors', 'consultation', 'service', 'vaccination', 'adults', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'featured', 'lasting', 'power', 'attorney', 'lpa', 'review', 'executive', 'ladies', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'try', 'natural', 'holistic', 'approach', 'wellbeing', 'chinese', 'medicine', 'today', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'featured', 'bone', 'calcium', 'tablets', 'raffles', 'adult', 'multis', 'tablets', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'orders', 'supplements', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'featured', 'save', 'rymco', 'surgical', 'mask', 'new', 'turmeric', 'curcumin', 'tablets', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'clickhereto', 'view', 'latest', 'issue', 'term', 'wave', 'enewsletter', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'sir', 'stamford', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'inaugurated', 'mass', 'reforms', 'aimed', 'transforming', 'dutch', 'colonial', 'system', 'improving', 'condition']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'entrusted', 'independent', 'authority', 'aroused', 'jealousy', 'penang', 'established', 'headquarters', 'malacca', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'rewarded', 'extraordinary', 'work', 'appointment', 'mintos', 'staff', 'sailed', 'java', 'expeditionary', 'force', 'landed', 'without', 'mishap', 'aug', 'short', 'sharp']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'minto', 'gave', 'considerable', 'credit', 'success', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'shortly', 'afterward', 'minto', 'sailed', 'calcutta', 'leaving', 'age', 'rule', 'java', 'also', 'archipelagic', 'empire', 'several', 'million', 'inhabitants', '<pad>']\n",
      "['<pad>', 'born', 'improvident', 'merchant', 'captain', 'wife', 'homeward', 'voyage', 'west', 'indies', 'grew', 'atmosphere', 'debt', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'however', 'time', 'rapidly', 'deteriorating', 'health', 'characterized', 'headaches', 'increasing', 'ferocity', 'sailed']\n",
      "['<pad>', '<pad>', 'penang', 'established', 'give', 'britain', 'foothold', 'dutchheld', 'east', 'indies', 'shaped', 'career', 'intensive', 'exploration', 'language', 'history', 'culture', 'malayan', 'peoples', 'scattered']\n",
      "['<pad>', '<pad>', '<pad>', 'determined', 'remove', 'java', 'french', 'influence', 'minto', 'appointed', 'agent', 'prepare', 'way', 'naval', 'invasion', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'sir', 'stamford', 'full', 'sir', 'thomas', 'stamford', 'raffles', 'born', 'july', 'sea', 'port', 'morant']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'singapore', 'hotel', 'singapore', 'deals', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'iconic', 'singapore', 'provides', 'relaxing', 'getaway', 'heart', 'city', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'prices', 'singapore', 'may', 'vary', 'depending', 'stay', 'eg', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'lock', 'great', 'price', 'singapore', 'rated', 'recent', 'guests', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'checkin', 'singapore', 'pm', 'checkout', 'pm', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'see', 'travelers', 'asked', 'staying', 'hotel', 'singapore', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'would', 'like', 'stay', 'singapore', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'room', 'options', 'singapore', 'include', 'suite', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'please', 'inform', 'singapore', 'expected', 'arrival', 'time', 'advance', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'nearest', 'airport', 'get', 'singapore', 'taxi', 'min', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['may', 'wish', 'book', 'dining', 'reservations', 'hour', 'butlers', 'upon', 'arrival', 'advance', 'service', 'team', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'singapore', 'accepts', 'cards', 'reserves', 'right', 'temporarily', 'hold', 'amount', 'prior', 'arrival']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'singapore', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'residents', 'hotel', 'singapore', 'enjoy', 'complimentary', 'pool', 'access', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'singapore', 'accepts', 'cards', 'reserves', 'right', 'temporarily', 'hold', 'amount', 'prior', 'arrival']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'singapore', 'offers', 'following', 'activitiesservices', 'charges', 'may', 'apply', 'spafitness', 'centerhot', 'tubjacuzzisaunamassagepool']\n",
      "['<pad>', '<pad>', '<pad>', 'legendary', 'singapore', 'sling', 'available', 'historic', 'long', 'bar', 'arcade', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'guests', 'browse', 'souvenir', 'gift', 'shop', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'guests', 'staying', 'singapore', 'enjoy', 'highlyrated', 'breakfast', 'stay', 'guest', 'review', 'score', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'find', 'facilities', 'singapore', 'page', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', 'hotel', 'minute', 'walk', 'city', 'hall', 'mrt', 'station', 'city', 'shopping', 'center', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'singapore', 'welcoming', 'bookingcom', 'guests', 'since', 'oct', 'hotel', 'chainbrand', 'raffles', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'singapore', 'feet', 'center', 'singapore', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'following', 'parking', 'options', 'available', 'guests', 'staying', 'singapore', 'subject', 'availability', 'parking', 'siteprivate', 'parkingparkingsecure', 'parkingparking', 'garageaccessible', 'parkingfree', 'parking']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 89/89 [00:00<00:00, 9890.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'university', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', 'return', 'inperson', 'campus', 'life', 'may', 'prompt', 'anxiety', 'tips', 'university', 'psychology', 'professor', 'jane', 'simoni', 'help', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'university', 'seattle', 'wa', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'discover', 'best', 'restaurants', 'bars', 'dc', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'stay', 'current', 'things', 'around', 'dc', 'signing', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'take', 'advantage', 'dcs', 'numerous', 'free', 'events', 'museums', 'tours', 'attractions', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'travel', 'deals', 'discounts', 'dc', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'discover', 'best', 'things', 'dc', 'events', 'happening', 'right', 'annual', 'festivals', 'much', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'locals', 'know', 'difference', 'dc', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'ultimate', 'guide', 'autumn', 'dc', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'coronavirus', 'information', 'faqs', 'dc', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'discover', 'best', 'things', 'dc', 'events', 'happening', 'right', 'annual', 'festivals', 'much', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'experience', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'welcome', 'state', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'information', 'plan', 'next', 'trip', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'order', 'copy', 'official', 'state', 'visitors', 'guide', 'view', 'online', 'edition', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'check', 'facebook', 'page', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'welcome', 'official', 'travel', 'tourism', 'website', 'state', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'use', 'maps', 'official', 'state', 'visitors', 'guide', 'plan', 'next', 'vacation', 'pacific', 'northwest', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', 'click', 'view', 'digital', 'version', 'guide', 'click', 'read', 'state', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'stay', 'know', 'goingson', 'state', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 52/52 [00:00<00:00, 26057.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'located', 'northeastern', 'coast', 'singapore', 'pulau', 'land', 'area', 'hectares', 'singapores', 'largest', 'natural', 'offshore', 'island', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'originally', 'consisting', 'two', 'islands', 'pulau', 'besar', 'meaning', 'big', 'tekong', 'island', 'malay', 'pulau', 'tekong', 'kechil', 'meaning']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'pulau', 'home', 'dozen', 'villages', 'largest', 'singapores', 'natural', 'offshore', 'islands', 'holds', 'special']\n",
      "['<pad>', '<pad>', 'first', 'edition', 'memorial', 'halls', 'public', 'lecture', 'series', 'pulau', 'mapping', 'research', 'consultant', 'mok', 'ly', 'yng', 'share', 'history', 'tekong', 'island']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'join', 'mr', 'moks', 'talk', 'learn', 'transformation', 'years', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'home', 'residents', 'pulau', 'converted', 'military', 'training', 'base', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['singapore', 'adopt', 'dutch', 'polder', 'concept', 'new', 'land', 'reclamation', 'method', 'pulau', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', '<pad>', '<pad>', 'jointly', 'parties', 'ensured', 'design', 'polder', 'pulau', 'costeffective', 'safe', 'environmentallysensitive', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<pad>', 'singapore', 'reclaim', 'land', 'development', 'polder', 'northwestern', 'tip', 'pulau', 'island', 'northeast', 'mainland', 'singapore', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['wong', 'singapores', 'minister', 'national', 'development', 'second', 'minister', 'finance', 'visit', 'pulau', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', 'deltares', 'worked', 'closely', 'hdb', 'since', 'feasibility', 'polder', 'concept', 'pulau', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#cleaning the crawled text for inference\n",
    "# cleaning\n",
    "for inference_target in ['NYC','obama','raffles','washington','tekong']:\n",
    "    inference_file = f\"./inference/{inference_target}.txt\"\n",
    "    inference_output_file = f\"./inference/clean_{inference_target}.txt\"\n",
    "    with open(inference_output_file,'w') as o:\n",
    "        with open(inference_file,'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            line_written =0\n",
    "            for line in tqdm(lines):\n",
    "                result = extract(line,inference_target,10,\"<pad>\",True)\n",
    "                if result == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    print(result)\n",
    "                    o.write(f'{inference_target}:')\n",
    "                    o.write(''.join([i+' ' for i in result] ))\n",
    "                    o.write('\\n')\n",
    "                    line_written += 1\n",
    "                    if line_written % 10000 == 0:\n",
    "                        print(f\"Written {line_written} into file {inference_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61d61677-8180-4f4f-b8ba-0712ff1acd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference batch: sian\n",
      "\n",
      "\n",
      "(400000,)\n",
      "\n",
      "                               Word       Unnormalized Cosine distance\n",
      "\n",
      "---------------------------------------------------------\n",
      "\n",
      "0                              tired\t\t0.531570\n",
      "1                             scared\t\t0.414386\n",
      "2                              weary\t\t0.412458\n",
      "3                                 'm\t\t0.400601\n",
      "4                         frustrated\t\t0.399057\n",
      "5                          exhausted\t\t0.392542\n",
      "6                             afraid\t\t0.389355\n",
      "7                               feel\t\t0.388964\n",
      "8                              bored\t\t0.386259\n",
      "9                           fatigued\t\t0.386248\n",
      "\n",
      "\n",
      "\t\tCosim score: [0.15159323]\n"
     ]
    }
   ],
   "source": [
    "with open(inference_output_file,'r') as f:\n",
    "    batch = f.readlines()\n",
    "    \n",
    "sample_batch_tensor = []\n",
    "target_batch_tensor = []\n",
    "for sentence in batch:\n",
    "    y,x = sentence.split(':')\n",
    "    x = re.sub('[\\n\\r\\ ]+',' ',x).strip()\n",
    "    sample_tensor = [get_glove_vec(word,W_norm,vocab) for word in x.split(' ')]\n",
    "    target_batch_tensor.append(get_glove_vec(y,W_norm,vocab))\n",
    "    sample_batch_tensor.append(sample_tensor)\n",
    "    \n",
    "sample_batch_tensor = torch.Tensor(np.array(sample_batch_tensor))\n",
    "target_batch_tensor = np.array(target_batch_tensor)\n",
    "\n",
    "sample_output = model(sample_batch_tensor)\n",
    "\n",
    "output = torch.mean(sample_output,0)   #sum across embeddings\n",
    "infer_vec_output = output.detach().numpy().reshape((1,100))\n",
    "\n",
    "print(f\"Inference batch: {inference_target}\\n\\n\")\n",
    "\n",
    "__distance(W_norm,vocab,ivocab,infer_vec_output)\n",
    "\n",
    "\n",
    "print(f\"\\n\\n\\t\\tCosim score: {cosim(infer_vec_output,target_label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c796e-aa08-4f5d-9553-a51654f7a3b7",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"s6\">Section 6: Incorporating synthesis vector back into GloVe</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "fa8f86c7-32e4-4733-870f-552f4f23da80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "400000\n",
      "['sian'] []\n"
     ]
    }
   ],
   "source": [
    "## section 6: writing the vectors back \n",
    "\n",
    "\n",
    "#choose an output vec that performs well \n",
    "output_vec = infer_vec_output\n",
    "embeddings = {inference_target:output_vec}  #word: embedding\n",
    "\n",
    "original_glove = './glove_6B/glove.6B.100d.txt'\n",
    "modified_glove = './glove_6B/modded_glove.6B.100d.txt'\n",
    "\n",
    "#the word might have existed in the corpus, so we will play safe\n",
    "existing = []\n",
    "with open(modified_glove,'w',encoding='utf-8') as f:\n",
    "    with open(original_glove,'r',encoding='utf-8') as i:\n",
    "        data = i.readlines()\n",
    "        for line in data:\n",
    "            word = line.rstrip().split(' ')[0]\n",
    "            if word in embeddings:  \n",
    "                f.write(f\"{word} \")  \n",
    "                f.write(' '.join([f'{i:.5f}' for i in embeddings[word].flatten().tolist()]))\n",
    "                f.write('\\n')\n",
    "                existing.append(word)\n",
    "            else:\n",
    "                f.write(line)\n",
    "        non_existing = [i for i in embeddings.keys() if i not in existing]\n",
    "        for new_word in non_existing:\n",
    "            f.write(f\"{word} \")  \n",
    "            f.write(' '.join([f'{i:.5f}' for i in embeddings[word].flatten().tolist()]))\n",
    "            f.write('\\n')\n",
    "\n",
    "#checking if it has been incorporated\n",
    "with open(modified_glove,'r',encoding='utf-8') as f:\n",
    "    temp = f.readlines()\n",
    "print(len(temp))\n",
    "(m_W_norm, mod_vocab, m_ivocab) = generate(modified_glove)\n",
    "print(len(mod_vocab))\n",
    "print(existing,non_existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4e57d-e2cf-47a1-b29e-a93a1f0b4620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
